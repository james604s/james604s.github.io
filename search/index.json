[{"content":"PySpark Data Lineage with Datahub åœ¨æˆ‘å€‘æ—¥å¸¸çš„è³‡æ–™å·¥ä½œä¸­ï¼Œä¸€å€‹æœ€ä»¤äººé ­ç—›çš„å•é¡Œè«éæ–¼ï¼šã€Œé€™å¼µå ±è¡¨çš„è³‡æ–™åˆ°åº•å¾å“ªè£¡ä¾†ï¼Ÿä¸­é–“ç¶“éäº†å“ªäº›è™•ç†ï¼Ÿã€ç•¶è³‡æ–™æµç¨‹è®Šå¾—è¶Šä¾†è¶Šè¤‡é›œæ™‚ï¼Œå¦‚æœæ²’æœ‰æ¸…æ™°çš„è³‡æ–™è¡€ç·£ (Data Lineage)ï¼Œè¿½è¹¤å•é¡Œã€è©•ä¼°è®Šæ›´å½±éŸ¿ï¼Œç”šè‡³ç¢ºä¿è³‡æ–™çš„å¯ä¿¡åº¦ï¼Œéƒ½æœƒè®Šæˆä¸€å ´å™©å¤¢ã€‚\nå¹¸é‹çš„æ˜¯ï¼Œåƒ PySpark é€™æ¨£çš„å¼·å¤§è™•ç†å¼•æ“ï¼Œæ­é… Datahub é€™æ¨£çš„ç¾ä»£è³‡æ–™ç›®éŒ„ (Data Catalog)ï¼Œå¯ä»¥å¹«åŠ©æˆ‘å€‘è‡ªå‹•åŒ–åœ°è§£é–‹é€™å€‹è¬åœ˜ã€‚\nåœ¨ Datahub ä¸­å‘ˆç¾çš„æ¸…æ™°è³‡æ–™è¡€ç·£åœ–\nSpark \u0026amp; Lineage åœ¨æˆ‘å€‘å‹•æ‰‹å¯«ä»»ä½•ç¨‹å¼ç¢¼ä¹‹å‰ï¼Œæœ€é‡è¦çš„ä¸€æ­¥æ˜¯ç†è§£é€™èƒŒå¾Œçš„ã€Œé­”æ³•ã€æ˜¯å¦‚ä½•é‹ä½œçš„ã€‚PySpark æœ¬èº«ä¸¦ä¸æœƒä¸»å‹•å‘Šè¨´å¤–ç•Œå®ƒçš„è³‡æ–™æµå‘ï¼Œé‚£éº¼ Datahub æ˜¯å¦‚ä½•å¾—çŸ¥çš„å‘¢ï¼Ÿ\nç­”æ¡ˆå°±åœ¨æ–¼ Spark æä¾›çš„ä¸€å€‹å¼·å¤§æ©Ÿåˆ¶ï¼šSparkListener ä»‹é¢ã€‚\nSparkListenerï¼šå®‰æ’åœ¨ Spark å…§çš„äº‹ä»¶ç›£è½å™¨ SparkListener æ˜¯ Spark æ¡†æ¶ä¸­åŸºæ–¼è§€å¯Ÿè€…æ¨¡å¼ (Observer Pattern) çš„ä¸€å€‹å¯¦ä½œã€‚ä½ å¯ä»¥æŠŠå®ƒæƒ³åƒæˆä¸€å€‹æˆ‘å€‘å®‰æ’åœ¨ Spark æ‡‰ç”¨ç¨‹å¼å…§éƒ¨çš„ã€Œäº‹ä»¶ç›£è½å™¨ã€ã€‚ç•¶ä½ çš„ PySpark è…³æœ¬åŸ·è¡Œæ™‚ï¼Œå®ƒå…§éƒ¨çš„æ¯ä¸€å€‹é‡è¦å‹•ä½œâ€”â€”ä½œæ¥­é–‹å§‹ (onJobStart)ã€ä½œæ¥­çµæŸ (onJobEnd)ï¼Œä¹ƒè‡³æ–¼ä¸€å€‹æŸ¥è©¢çš„åŸ·è¡Œ (onQueryExecution)â€”â€”éƒ½æœƒåœ¨ Spark å…§éƒ¨è§¸ç™¼ä¸€å€‹å°æ‡‰çš„ã€Œäº‹ä»¶ã€ã€‚\næˆ‘å€‘çš„è¡€ç·£æ“·å–å·¥å…·ï¼Œæ­£æ˜¯é€éå¯¦ä½œé€™å€‹ä»‹é¢ä¾†ç›£è½é€™äº›äº‹ä»¶ã€‚å…¶ä¸­ï¼Œå°æ–¼è³‡æ–™è¡€ç·£ä¾†èªªï¼Œæœ€é‡è¦çš„äº‹ä»¶æ˜¯ onQueryExecutionã€‚\nLogical Planï¼šSpark ä»»å‹™çš„åŸå§‹è¨­è¨ˆè—åœ– ç•¶ onQueryExecution äº‹ä»¶è¢«è§¸ç™¼æ™‚ï¼Œç›£è½å™¨æœƒæ”¶åˆ°ä¸€å€‹ QueryExecution ç‰©ä»¶ï¼Œé€™å€‹ç‰©ä»¶ä¸­åŒ…å«äº†æˆ‘å€‘æ“·å–è¡€ç·£çš„é—œéµè³‡è¨Šï¼šé‚è¼¯è¨ˆåŠƒ (Logical Plan)ã€‚\nLogical Plan æ˜¯ä¸€å€‹æŠ½è±¡èªæ³•æ¨¹ (Abstract Syntax Tree, AST)ï¼Œå®ƒä»£è¡¨äº†ä½ çš„ DataFrame æ“ä½œæˆ– SQL æŸ¥è©¢åœ¨ç¶“éä»»ä½•å„ªåŒ–ä¹‹å‰çš„å®Œæ•´çµæ§‹ã€‚ç‚ºä»€éº¼å¼·èª¿ã€Œæœªç¶“å„ªåŒ–ã€ï¼Ÿå› ç‚ºå®ƒæœ€å¿ å¯¦åœ°åæ˜ äº†ä½ ç·¨å¯«çš„ç¨‹å¼ç¢¼é‚è¼¯ã€‚é€™å€‹è¨ˆç•«è©³ç´°è¨˜éŒ„äº†ï¼š\nè³‡æ–™ä¾†æº (Sources)ï¼šé€é UnresolvedRelation ç­‰ç¯€é»ï¼Œæ¨™ç¤ºè³‡æ–™æ˜¯å¾å“ªå€‹è³‡æ–™åº«çš„å“ªå¼µè¡¨è®€å–çš„ã€‚ è½‰æ›æ“ä½œ (Transformations)ï¼šé€é Project (å°æ‡‰ select æˆ– withColumn)ã€Filterã€Join ç­‰ç¯€é»ï¼Œæè¿°è³‡æ–™ç¶“éçš„æ¯ä¸€å€‹è™•ç†æ­¥é©Ÿã€‚ è³‡æ–™å»å‘ (Sink)ï¼šæè¿°æœ€çµ‚çµæœè¢«å¯«å…¥åˆ°å“ªå€‹ç›®çš„åœ°ã€‚ è¡€ç·£å·¥å…·çš„æ ¸å¿ƒä»»å‹™ï¼Œå°±æ˜¯éè¿´åœ°éæ­·é€™æ£µ Logical Plan æ¨¹ï¼Œè§£æå‡ºå…¶ä¸­åŒ…å«çš„è¼¸å…¥ã€è¼¸å‡ºä»¥åŠæ¬„ä½ç´šåˆ¥çš„æ“ä½œï¼Œå¾è€Œå®Œæ•´åœ°é‚„åŸå‡ºè³‡æ–™çš„å®Œæ•´æ—…ç¨‹ã€‚\næ ¸å¿ƒæ¦‚å¿µï¼šç„¡è«–æ˜¯ Datahub çš„åŸç”Ÿæ•´åˆå·¥å…·ï¼Œé‚„æ˜¯ OpenLineage çš„é–‹æ”¾æ¨™æº–å·¥å…·ï¼Œå®ƒå€‘çš„æ ¸å¿ƒéƒ½æ˜¯å¯¦ç¾äº†ä¸€å€‹ SparkListenerã€‚å®ƒå€‘çš„æ ¹æœ¬å·®ç•°ï¼Œä¸åœ¨æ–¼å¦‚ä½•æ“·å– Logical Planï¼Œè€Œåœ¨æ–¼å¦‚ä½•å°‡è§£æå¾Œçš„è¡€ç·£è³‡è¨Šã€Œæ ¼å¼åŒ–ã€ä¸¦ã€ŒåŒ¯å ±ã€çµ¦ Datahubã€‚\nETL Example (éƒ¨åˆ†ç”± Gemini ç”Ÿæˆ) ç†è«–è¬›å®Œäº†ï¼Œè®“æˆ‘å€‘ä¾†å»ºç«‹ä¸€å€‹çµ±ä¸€çš„å¯¦æˆ°å ´æ™¯ã€‚ä»¥ä¸‹æ˜¯ä¸€å€‹é€šç”¨çš„ PySpark è…³æœ¬ï¼Œå®ƒå°‡æ˜¯æˆ‘å€‘å¾ŒçºŒå…©ç¨®æ–¹æ³•çš„æ¸¬è©¦å°è±¡ã€‚\nä»»å‹™ç›®æ¨™ï¼šå¾ MSSQL çš„ dbo.source_table è®€å–è³‡æ–™ï¼Œé€²è¡Œä¸€ç³»åˆ—è½‰æ›ï¼Œç„¶å¾Œå°‡çµæœå¯«å…¥ dbo.target_tableã€‚\nprocess_mssql_data.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from pyspark.sql import SparkSession from pyspark.sql.functions import col, lit, current_timestamp, upper, year, month def main(): \u0026#34;\u0026#34;\u0026#34; ä¸€å€‹å¾ MSSQL è®€å–ã€è½‰æ›ä¸¦å¯«å…¥çš„ Spark ETL ä½œæ¥­ã€‚ \u0026#34;\u0026#34;\u0026#34; spark = SparkSession.builder.appName(\u0026#34;MSSQL_ETL_Lineage_Demo\u0026#34;).getOrCreate() # db conn jdbc_url = \u0026#34;jdbc:sqlserver://your_mssql_server:1433;databaseName=your_db\u0026#34; connection_properties = { \u0026#34;user\u0026#34;: \u0026#34;your_user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;your_password\u0026#34;, \u0026#34;driver\u0026#34;: \u0026#34;com.microsoft.sqlserver.jdbc.SQLServerDriver\u0026#34; } source_table = \u0026#34;dbo.source_table\u0026#34; target_table = \u0026#34;dbo.target_table\u0026#34; # Read Table print(\u0026#34;--- æ­£åœ¨å¾ä¾†æºè³‡æ–™è¡¨è®€å–è³‡æ–™ ---\u0026#34;) source_df = spark.read.jdbc( url=jdbc_url, table=source_table, properties=connection_properties ) print(\u0026#34;ä¾†æº DataFrame Schema:\u0026#34;) source_df.printSchema() # Tranform print(\u0026#34;--- æ­£åœ¨é€²è¡Œè³‡æ–™è½‰æ› ---\u0026#34;) transformed_df = source_df \\ .withColumn(\u0026#34;processed_at\u0026#34;, current_timestamp()) \\ .withColumn(\u0026#34;job_name\u0026#34;, lit(\u0026#34;MSSQL_ETL_Lineage_Demo\u0026#34;)) \\ .withColumn(\u0026#34;source_system_upper\u0026#34;, upper(col(\u0026#34;source_system\u0026#34;))) \\ .withColumn(\u0026#34;event_year\u0026#34;, year(col(\u0026#34;event_date\u0026#34;))) \\ .withColumn(\u0026#34;event_month\u0026#34;, month(col(\u0026#34;event_date\u0026#34;))) \\ .filter(col(\u0026#34;value\u0026#34;) \u0026gt; 100) print(\u0026#34;è½‰æ›å¾Œ DataFrame Schema:\u0026#34;) transformed_df.printSchema() # Write print(f\u0026#34;--- æ­£åœ¨å°‡è³‡æ–™å¯«å…¥ {target_table} ---\u0026#34;) transformed_df.write.jdbc( url=jdbc_url, table=target_table, mode=\u0026#34;overwrite\u0026#34;, properties=connection_properties ) print(\u0026#34;--- ä½œæ¥­æˆåŠŸå®Œæˆï¼ ---\u0026#34;) spark.stop() if __name__ == \u0026#34;__main__\u0026#34;: main() ç¾åœ¨ï¼Œæˆ‘å€‘å°‡ç”¨å…©ç¨®ä¸åŒçš„ spark-submit æŒ‡ä»¤ä¾†åŸ·è¡ŒåŒä¸€å€‹è…³æœ¬ï¼Œçœ‹çœ‹å®ƒå€‘æ˜¯å¦‚ä½•å¯¦ç¾è¡€ç·£è¿½è¹¤çš„ã€‚\nè·¯å¾‘ Aï¼šç›´é”è»Š - Datahub åŸç”Ÿæ•´åˆ (acryl-spark-lineage) é€™æ¢è·¯å¾‘åƒæ˜¯æ­ä¹˜ä¸€ç­ç‚º Datahub é‡èº«æ‰“é€ çš„ç›´é”å°ˆè»Šï¼Œç°¡å–®ã€å¿«é€Ÿï¼Œæ”¹é€  OpenLineage æ¨™æº–ä»¥ç¬¦åˆ Datahub çš„æ ¼å¼ã€‚\nå ±å‘Šæ ¼å¼ï¼šå°‡ Logical Plan ç›´æ¥ç¿»è­¯æˆ Datahub å…§éƒ¨èƒ½ç†è§£çš„æ ¼å¼ï¼Œç¨±ç‚º Metadata Change Proposals (MCPs)ã€‚ å ±å‘Šæ©Ÿåˆ¶ï¼šé€é HTTP API ç›´æ¥å°‡é€™äº› MCPs ç™¼é€çµ¦ Datahub çš„æ ¸å¿ƒæœå‹™ (GMS)ã€‚ ç¢ºä¿ä½ å·²ç¶“ä¸‹è¼‰äº† MSSQL JDBC é©…å‹•\n1 export MSSQL_JDBC_JAR=\u0026#34;/path/to/mssql-jdbc.jar\u0026#34; spark-submit æŒ‡ä»¤ï¼š\n1 2 3 4 5 6 7 spark-submit \\ --master local[*] \\ --packages io.acryl:acryl-spark-lineage_2.12:0.2.18 \\ --jars ${MSSQL_JDBC_JAR} \\ --conf \u0026#34;spark.extraListeners=io.acryl.spark.AcrylSparkListener\u0026#34; \\ --conf \u0026#34;spark.datahub.url=http://your-datahub-gms-host:8080\u0026#34; \\ --conf \u0026#34;spark.datahub.token=YOUR_DATAHUB_API_TOKEN\u0026#34; \\ 1 process_mssql_data.py spark.extraListeners: é€™æ˜¯å•Ÿå‹•é­”æ³•çš„é—œéµï¼Œå‘Šè¨´ Spark è¦æ›è¼‰ Acryl çš„ç›£è½å™¨ã€‚ spark.datahub.url: Datahub GMS æœå‹™çš„ç«¯é» (Endpoint)ã€‚ è·¯å¾‘ Bï¼šè‡ªç”±è¡Œ - OpenLineage é–‹æ”¾æ¨™æº– é€™æ¢è·¯å¾‘æ¡ç”¨äº† OpenLineage é€™å€‹é–‹æ”¾æ¨™æº–ï¼Œæä¾›äº†æ›´å¤§çš„éˆæ´»æ€§å’Œæœªä¾†æ“´å……æ€§ã€‚\nå ±å‘Šæ ¼å¼ï¼šå°‡ Logical Plan ç¿»è­¯æˆä¸€ç¨®é€šç”¨çš„ã€æ¨™æº–åŒ–çš„ OpenLineage Event (JSON æ ¼å¼)ã€‚é€™ä»½ã€Œå ±å‘Šã€ä»»ä½•æ”¯æ´ OpenLineage çš„å¹³å°éƒ½çœ‹å¾—æ‡‚ã€‚ å ±å‘Šæ©Ÿåˆ¶ï¼šå°‡æ¨™æº–åŒ–çš„ JSON äº‹ä»¶ç™¼é€åˆ°ä¸€å€‹æŒ‡å®šçš„ HTTP ç«¯é»ã€‚Datahub æ°å¥½æä¾›äº†ä¸€å€‹é€™æ¨£çš„ç«¯é»ä¾†æ¥æ”¶é€™äº›äº‹ä»¶ã€‚ 1 export MSSQL\\_JDBC\\_JAR=\u0026#34;/path/to/mssql-jdbc.jar\u0026#34; spark-submit æŒ‡ä»¤ï¼š\n1 2 3 4 5 6 7 8 spark-submit \\ --master local[*] \\ --packages io.openlineage:openlineage-spark:1.13.0 \\ --jars ${MSSQL_JDBC_JAR} \\ --conf \u0026#34;spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\u0026#34; \\ --conf \u0026#34;spark.openlineage.transport.type=http\u0026#34; \\ --conf \u0026#34;spark.openlineage.transport.url=http://your-datahub-gms-host:8080/api/v1/lineage\u0026#34; \\ --conf \u0026#34;spark.openlineage.namespace=my_production_etl\u0026#34; \\ 1 process_mssql_data.py spark.openlineage.transport.url: è¨­å®šæ¥æ”¶ç«¯é»ï¼Œæ³¨æ„ï¼é€™è£¡æŒ‡å‘çš„æ˜¯ Datahub å°ˆé–€ç”¨ä¾†æ¥æ”¶ OpenLineage äº‹ä»¶çš„ API ç«¯é»ã€‚ å¦‚ä»¥ä¸Šéƒ½åŸ·è¡ŒæˆåŠŸï¼Œé è¨ˆå¯ä»¥æŠ“åˆ°è³‡æ–™è¡€ç·£ (Table to Table)ã€(Column to Column)ï¼Œ å»ºè­°å¯ä»¥ Dev æœŸé–“å¯ä»¥å…ˆé€é Jupyter Notebook ä¸€è¡ŒåŸ·è¡Œï¼Œ Prod å†é€²è¡Œ Spark Submitã€‚\nSummary æ–‡ç« ä¸­æ¢è¨äº†å¦‚ä½•é€é PySpark è‡ªå‹•åŒ–åœ°å°‡è³‡æ–™è¡€ç·£å‚³é€åˆ° Datahubã€‚\næ ¸å¿ƒå•é¡Œï¼šè¤‡é›œçš„è³‡æ–™æµç¨‹å°è‡´å¯è§€æ¸¬æ€§ (Observability) ä½è½ï¼Œé›£ä»¥è¿½è¹¤å’Œç®¡ç†ã€‚ è§£æ±ºæ–¹æ¡ˆï¼šåˆ©ç”¨ Spark å…§å»ºçš„ SparkListener æ©Ÿåˆ¶ï¼Œåœ¨åŸ·è¡ŒæœŸé–“æ“·å– Logical Planï¼Œå¾è€Œè‡ªå‹•ç”Ÿæˆè¡€ç·£é—œä¿‚ã€‚ å…©ç¨®å¯¦ç¾è·¯å¾‘ï¼š Datahub åŸç”Ÿæ•´åˆ (acryl-spark-lineage)ï¼šæœ€ç°¡å–®ã€æœ€ç›´æ¥çš„æ–¹æ³•ï¼Œå°ˆç‚º Datahub è¨­è¨ˆï¼Œé©åˆè¿½æ±‚å¿«é€Ÿå¯¦æ–½ä¸”æŠ€è¡“æ£§å–®ä¸€çš„åœ˜éšŠã€‚ç›®å‰åŠŸèƒ½ä¸Šæ•´åˆè¶Šä¾†è¶Šå¤š OpenLineage çš„ Feature åšä½¿ç”¨ã€‚ OpenLineage é–‹æ”¾æ¨™æº–ï¼šæ›´å…·éˆæ´»æ€§å’Œæ“´å……æ€§çš„æ–¹æ³•ï¼Œå®ƒå°‡è¡€ç·£è³‡è¨Šæ¨™æº–åŒ–ï¼Œä½¿ä½ çš„æ¶æ§‹ä¸å—ç‰¹å®šå» å•†ç¶å®šï¼Œæ˜¯å»ºç«‹ä¼æ¥­ç´šã€å¯æŒçºŒæ¼”é€²è³‡æ–™å¹³å°çš„é¦–é¸ã€‚ å¦‚ä½•é¸æ“‡ï¼Ÿ\nç‰¹æ€§ Datahub åŸç”Ÿæ•´åˆ (è·¯å¾‘ A) OpenLineage æ¨™æº– (è·¯å¾‘ B) è¨­å®šé›£åº¦ â­ (æ¥µç°¡) â­â­ (ç¨æœ‰é–€æª») å» å•†ä¸­ç«‹æ€§ âŒ (ç¶å®š Datahub) âœ… (å¯ç™¼é€åˆ°ä»»ä½•ç›¸å®¹å¾Œç«¯) é™¤éŒ¯èƒ½åŠ› â­â­ (ä¸­ç­‰) â­â­â­â­â­ (æ¥µä½³ï¼Œå¯ç›´æ¥æŸ¥çœ‹äº‹ä»¶å…§å®¹) é•·æœŸæ¶æ§‹ é©åˆå–®ä¸€å¹³å°ã€å¿«é€Ÿå¯¦æ–½ é©åˆä¼æ¥­ç´šã€å¯æŒçºŒæ¼”é€²çš„å¹³å° Reference Datahub Native Integration Official Datahub Spark Lineage GitHub: acryl-spark-lineage OpenLineage Standard Official OpenLineage OpenLineage Spark Integration GitHub: openlineage-spark ","date":"2025-09-27T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_lineage_with_datahub/","title":"PySpark å­¸ç¿’ç­†è¨˜ - Data Lineage with Datahub"},{"content":"PySpark - DataFrame Writer èˆ‡ Partition Example Overview åƒæ•¸/æ–¹æ³• ä½œç”¨ å°è¼¸å‡ºæª”æ¡ˆæ•¸ å°Folderçµæ§‹ å°å¾ŒçºŒæŸ¥è©¢æ•ˆèƒ½ å…¸å‹ç”¨é€” repartition(N) é‡æ–°Partitionï¼ˆéš¨æ©Ÿï¼‰ æ§åˆ¶ï¼ˆâ‰ˆN æª”ï¼‰ ç„¡å½±éŸ¿ ç„¡ç›´æ¥å¹«åŠ© æ§åˆ¶æª”æ¡ˆæ•¸/å¹³è¡¡åˆ†å·¥ partitionBy(colsâ€¦) ä¾æ¬„ä½å€¼åˆ†Folder è¦–Partitionå€¼è€Œå®š å»ºç«‹ col=value/ å¼·ï¼šPartition Pruning æ™‚é–“åºåˆ—ã€ç¶­åº¦ç¯©é¸ bucketBy(B, colsâ€¦)* Hash åˆ†æ¡¶ï¼ˆè¡¨ç´šï¼‰ è¦–Bucketæ•¸èˆ‡è¨ˆç®—è€Œå®š ç„¡ï¼ˆè¡¨å…§é‚è¼¯åˆ†æ¡¶ï¼‰ ä¸­ï½å¼·ï¼šJoin/GroupBy æ¸›å°‘ Shuffle å¤§è¡¨ Join/èšåˆ sortBy(colsâ€¦)* Bucket/Partitionå…§æ’åº ç„¡ ç„¡ åŠ é€ŸBucketå…§æƒæ æ™‚åºæª¢ç´¢ã€ç¯„åœæŸ¥è©¢ option(\u0026quot;maxRecordsPerFile\u0026quot;, N) æ¯æª”ä¸Šé™ç­†æ•¸ åˆ‡æª”ï¼ˆâ‰¤N/æª”ï¼‰ ç„¡ ç„¡ç›´æ¥å¹«åŠ© é¿å…å°æª”/å·¨æª” * bucketBy/sortBy åªå° saveAsTable ç”Ÿæ•ˆï¼Œsave(path) ç„¡æ•ˆã€‚ Folder \u0026amp; Bucket 1 2 3 4 5 6 # partitionBy ä¹‹å¾Œçš„ Folder /data/out/ds=2025-09-07/part-0000.parquet /data/out/ds=2025-09-08/part-0001.parquet # bucketBy ä½œç”¨åœ¨ã€Œè¡¨ã€ï¼šæ²’æœ‰ Folder Layerè®ŠåŒ–ï¼Œä½†åœ¨ Metastore ä¸­è¨˜éŒ„ã€ŒBucketã€è³‡è¨Š db.bucketed_events --(16 buckets on user_id, sorted by event_ts) å¸¸è¦‹çµ„åˆèˆ‡è¼¸å‡ºæ•ˆæœ æ§åˆ¶æª”æ¡ˆæ•¸ 1 2 3 (df.repartition(32) # æ§åˆ¶è¼¸å‡º â‰ˆ 32 æª” .write.option(\u0026#34;maxRecordsPerFile\u0026#34;, 2000000) .parquet(\u0026#34;/data/out\u0026#34;)) æª”æ¡ˆæ•¸ï¼šâ‰ˆ 32ï½(æ›´å¤šï¼Œè‹¥æ¯æª”è¶…é N ç­†æœƒå†åˆ‡) æ•ˆèƒ½ï¼šç„¡Partitionä¿®å‰ªï¼›å–®ç´”æ§é¡†ç²’ã€‚ æ™‚é–“åºåˆ—æŸ¥è©¢ï¼ˆæœ€ä½³å¯¦è¸ï¼‰ 1 2 3 4 5 (df.write .partitionBy(\u0026#34;ds\u0026#34;) # ä»¥æ—¥æœŸåˆ†Folder .option(\u0026#34;maxRecordsPerFile\u0026#34;, 2_000_000) .mode(\u0026#34;append\u0026#34;) .parquet(\u0026#34;/lake/sales\u0026#34;)) Folderï¼š/lake/sales/ds=YYYY-MM-DD/... æŸ¥è©¢ï¼šWHERE ds='2025-09-07' åªæƒè©²æ—¥æœŸPartition â†’ å¿« å¤§è¡¨ Join/GroupByï¼ˆOLAP å ´æ™¯ï¼‰ 1 2 3 4 5 (df.write .mode(\u0026#34;overwrite\u0026#34;) .bucketBy(32, \u0026#34;user_id\u0026#34;) # Table Level Hash åˆ†æ¡¶ .sortBy(\u0026#34;event_ts\u0026#34;) .saveAsTable(\u0026#34;warehouse.bucketed_events\u0026#34;)) æ•ˆæœï¼šèˆ‡å¦ä¸€å¼µåŒBucketæ•¸ã€åŒ key çš„è¡¨ Join â†’ é¡¯è‘—æ¸›å°‘ Shuffle é™åˆ¶ï¼šåƒ… saveAsTableï¼›Bucketæ•¸å›ºå®šï¼Œæ”¹è®Šéœ€é‡å¯«è¡¨ã€‚ åªè¦†è“‹æŒ‡å®šPartitionï¼ˆé˜²æ­¢æ•´é«”è¦†è“‹ï¼‰ 1 2 3 4 5 6 spark.conf.set(\u0026#34;spark.sql.sources.partitionOverwriteMode\u0026#34;, \u0026#34;dynamic\u0026#34;) (df.filter(\u0026#34;ds=\u0026#39;2025-09-07\u0026#39;\u0026#34;) .write.partitionBy(\u0026#34;ds\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .parquet(\u0026#34;/lake/sales\u0026#34;)) çµæœï¼šåªè¦†è“‹ ds=2025-09-07 Partitionï¼Œä¸å‹•å…¶ä»–æ—¥æœŸã€‚ åƒæ•¸èª¿æ•´å»ºè­° éœ€æ±‚ æ¨è–¦åšæ³• å‚™è¨» æ¸›å°‘å°æª”/æ§åˆ¶æª”æ¡ˆæ•¸ repartition(N) + maxRecordsPerFile N å–æ±ºæ–¼å¢é›†/è³‡æ–™é‡ ç¯©é¸ç‚ºä¸»ï¼ˆæ—¥æœŸ/å€åŸŸï¼‰ partitionBy(\u0026quot;ds\u0026quot;, \u0026quot;region\u0026quot;) æ§åˆ¶Partitionæ¬„ä½åŸºæ•¸ï¼Œé¿å…çˆ†é‡Folder å¤§è¡¨é »ç¹ Join/GroupBy bucketBy(B, key)ï¼ˆ+ sortByï¼‰ åªç”¨ saveAsTableï¼›é›™è¡¨Bucketæ•¸/éµä¸€è‡´ æ™‚åºæƒæé‚„è¦å¿« partitionBy(\u0026quot;ds\u0026quot;) + åˆç†æª”æ¡ˆå¤§å° æ­é…ä¸‹æ¸¸æŸ¥è©¢æ¢ä»¶ä¸€è‡´ åš´æ ¼æ§åˆ¶è¦†è“‹ç¯„åœ partitionOverwriteMode=dynamic åªè¦†è“‹å¯«å…¥åˆ°çš„Partition æ³¨æ„äº‹é … bucketBy/sortBy å° save(path) ç„¡æ•ˆï¼›å¿…é ˆ saveAsTableï¼ˆHive/Glue/Spark Catalogï¼‰ã€‚ overwrite åœ¨Partitionè³‡æ–™é›†ä¸Šè‹¥æœªè¨­å®š partitionOverwriteMode=dynamicï¼Œå¯èƒ½æŠŠæ•´å€‹ç›®æ¨™è·¯å¾‘è¦†è“‹æ‰ã€‚ partitionBy é¸é«˜åŸºæ•¸æ¬„ä½ï¼ˆå¦‚ user_idï¼‰æœƒå°è‡´Partitionçˆ†ç‚¸èˆ‡å¤§é‡å°æª”æ¡ˆã€‚ repartition æœƒ Shuffleï¼›åœ¨è¶…å¤§è³‡æ–™é›†ä¸Šè¦ç•™æ„æˆæœ¬ã€‚ maxRecordsPerFile åªæ§åˆ¶ã€Œæ¯æª”ç­†æ•¸ã€ï¼Œä¸æ§åˆ¶ã€Œæª”æ¡ˆå¤§å°ã€ï¼›ä¸åŒæ ¼å¼/å£“ç¸®æ¯”æœƒæœ‰å·®ç•°ã€‚ ç¯„ä¾‹ ç¯„ä¾‹è³‡æ–™é›†ç‚º Udemy èª²ç¨‹ä¸­æä¾›çš„èˆªç­æ™‚é–“è³‡æ–™é›†\nè¨­å®šèˆ‡è®€å– Source Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from pyspark.sql import * from pyspark.sql.functions import spark_partition_id from lib.logger import Log4j # å¦‚ä½¿ç”¨åˆ° avro è¨˜å¾—è¦å»å®˜æ–¹ä¸‹è¼‰ä¸¦ç¢ºèªå°æ‡‰ç‰ˆæœ¬ # ç­†è€…ä½¿ç”¨ Scala 2.13, spark 3.4.3 spark = SparkSession \\ .builder \\ .master(\u0026#34;local[3]\u0026#34;) \\ .appName(\u0026#34;SparkSchemaDemo\u0026#34;) \\ .config(\u0026#34;spark.jars\u0026#34;, \u0026#34;/Users/squid504s/leonard_github/PySpark-Capstone/packages/spark-avro_2.13-3.4.3.jar\u0026#34;) \\ .getOrCreate() logger = Log4j(spark) flightTimeParquetDF = spark.read \\ .format(\u0026#34;parquet\u0026#34;) \\ .load(\u0026#34;dataSource/flight*.parquet\u0026#34;) Default æƒ…æ³ ç„¡ Repartition / ç„¡ PartitionBy é è¨­æƒ…æ³ä¸‹ â†’ å–®ä¸€åˆ†å€ å¯«å‡ºæ™‚åªæœƒç”¢ç”Ÿ 1 å€‹æª”æ¡ˆ æŸ¥è©¢æ™‚ç„¡æ³•é€²è¡Œåˆ†å€ä¿®å‰ª â†’ æ•ˆèƒ½è¼ƒå·® 1 2 3 4 5 6 7 8 9 logger.info(\u0026#34;Num Partitions before: \u0026#34; + str(flightTimeParquetDF.rdd.getNumPartitions())) flightTimeParquetDF.groupBy(spark_partition_id()).count().show() Result: +--------------------+------+ |SPARK_PARTITION_ID()| count| +--------------------+------+ | 0|470477| +--------------------+------+ ä½¿ç”¨ .repartition(5) â†’ æ§åˆ¶è¼¸å‡ºæª”æ¡ˆæ•¸ ç”¢ç”Ÿäº† 5 å€‹ Avro æª”æ¡ˆ ä½†é€™åªæ˜¯ éš¨æ©Ÿé‡æ–°åˆ†é…è³‡æ–™ â†’ ä¸æœƒç”¢ç”Ÿå¯¦é«” Partition Folder æŸ¥è©¢æ™‚ä»éœ€æƒææ‰€æœ‰æª”æ¡ˆï¼Œæ•ˆèƒ½æ²’å„ªåŒ– 1 2 3 4 5 6 7 8 9 10 11 12 13 partitionedDF = flightTimeParquetDF.repartition(5) logger.info(\u0026#34;Num Partitions after: \u0026#34; + str(partitionedDF.rdd.getNumPartitions())) partitionedDF.groupBy(spark_partition_id()).count().show() +--------------------+-----+ |SPARK_PARTITION_ID()|count| +--------------------+-----+ | 0|94096| | 1|94095| | 2|94095| | 3|94095| | 4|94096| +--------------------+-----+ 1 2 3 4 5 partitionedDF.write \\ .format(\u0026#34;avro\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;path\u0026#34;, \u0026#34;/Users/squid504s/leonard_github/PySpark-Capstone/05-DataSinkDemo/dataSinkTest/avro\u0026#34;) \\ .save() å¦‚æœæƒ³é‡å° èˆªç­é‹ç‡Ÿå•†(OP_CARRIER) èˆ‡ å‡ºç™¼åœ°(ORIGIN) å»ºç«‹å¯¦é«” Partiotionï¼Œå¯ä½¿ç”¨ .partitionBy() è®“è¼¸å‡ºæª”æ¡ˆæŒ‰æ¬„ä½å€¼åˆ† Folder Folder æœƒä¾ç…§ OP_CARRIER â†’ ORIGIN å»ºç«‹éšå±¤å¼çµæ§‹ æŸ¥è©¢æ™‚å¯ç›´æ¥é‡å°ç‰¹å®šé‹ç‡Ÿå•†æˆ–å‡ºç™¼åœ°åš Partition Pruning â†’ æ•ˆèƒ½å¤§å¹…æå‡ 1 2 3 4 5 6 7 8 9 flightTimeParquetDF.write \\ .format(\u0026#34;json\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;path\u0026#34;, \u0026#34;/Users/squid504s/leonard_github/PySpark-Capstone/05-DataSinkDemo/Avro_test/json/\u0026#34;) \\ .partitionBy(\u0026#34;OP_CARRIER\u0026#34;, \u0026#34;ORIGIN\u0026#34;) \\ .option(\u0026#34;maxRecordsPerFile\u0026#34;, 10000) \\ .save() spark.stop() Reference spark-avro package\nPySpark - Apache Spark Programming in Python for beginners\nApache Spark Official\n","date":"2025-09-20T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_df_writer_api_n_partition/","title":"PySpark å­¸ç¿’ç­†è¨˜ - DataFrame Writer èˆ‡ Partition Example"},{"content":"ğŸ”¥ PySpark - Spark Ã— Log4j2 æ—¥èªŒç®¡ç†èˆ‡é›†ä¸­åŒ– (é©ç”¨æ–¼ YARN / Livy / Standalone)\nSpark èˆ‡ Log4j çš„é—œä¿‚ é …ç›® èªªæ˜ æ—¥èªŒæŠ½è±¡å±¤ Spark ä½¿ç”¨ SLF4J ä½œç‚ºçµ±ä¸€æ—¥èªŒ APIã€‚ é è¨­æ—¥èªŒç³»çµ± Spark 3.2+ é è¨­æ¡ç”¨ Log4j2ï¼ˆèˆŠç‰ˆç‚º Log4j 1.xï¼‰ã€‚ ç”Ÿæ…‹ä¸€è‡´æ€§ Hadoopã€Hiveã€Kafkaã€HBase éƒ½ç”¨ Log4jï¼ŒSpark å¯ç„¡ç¸«æ•´åˆã€‚ ç”¨é€” æ§åˆ¶æ—¥èªŒç­‰ç´šã€è¼¸å‡ºæ ¼å¼ã€æª”æ¡ˆä½ç½®ã€è¼ªè½‰ç­–ç•¥ã€é›†ä¸­æ”¶é›†ã€‚ ç¯„åœ Spark Driverã€Executorã€History Serverã€Livy å‡æ”¯æ´çµ±ä¸€ç®¡ç†ã€‚ ç‚ºä»€éº¼å¾ Log4j å‡ç´šåˆ° Log4j2 ç‰¹æ€§ Log4j 1.x Log4j2 åœ¨ Spark çš„å¥½è™• æ•ˆèƒ½ åŒæ­¥å¯«å…¥ï¼Œæ•ˆèƒ½ä½ LMAX Disruptor (RingBuffer)ï¼Œæ•ˆèƒ½å¿« 10 å€ Executor å¤§é‡å¯« log ä¸é˜»å¡ AsyncAppender æ•ˆç‡ä½ åŸç”Ÿé«˜æ•ˆ AsyncAppender é«˜ä½µç™¼ ETL/Streaming ä»»å‹™æ›´ç©©å®š JSON æ”¯æ´ å¹¾ä¹ç„¡ åŸç”Ÿ JsonLayout é©åˆé›†ä¸­å¼æ—¥èªŒæ”¶é›† å‹•æ…‹èª¿æ•´ç­‰ç´š ä¸æ”¯æ´ æ”¯æ´ç†±æ›´æ–° Spark UI æˆ– REST API å³æ™‚åˆ‡æ› log level å®‰å…¨æ€§ å·²åœæ­¢ç¶­è­· æŒçºŒæ›´æ–° é¿å… Log4Shell é¡æ¼æ´ å¤š Appender é™åˆ¶è¼ƒå¤š æ”¯æ´ Consoleã€Fileã€JSONã€Socket åŒæ™‚è¼¸å‡ºå¤šä»½æ—¥èªŒ é©ç”¨æ€§ å°å‹æ‡‰ç”¨ åˆ†æ•£å¼å¢é›†å‹å–„ Executor å¤šæ™‚æ•ˆèƒ½ä½³ ç¸½çµ\nSpark 3.2+ é è¨­ä½¿ç”¨ Log4j2ï¼Œæ›´å®‰å…¨ã€æ›´é«˜æ•ˆï¼Œä¹Ÿé¿å…æ‰‹å‹•æ›¿æ›ç›¸ä¾æ€§å•é¡Œã€‚ è‹¥ä»åœ¨ä½¿ç”¨ Spark 2.xï¼ˆLog4j 1.xï¼‰ï¼Œå»ºè­°å‡ç´š Spark æˆ–æ›¿æ›ç‚º Log4j2ã€‚ Log4j2 æ ¸å¿ƒæ¦‚å¿µ å…ƒä»¶ åŠŸèƒ½ ç¯„ä¾‹ Logger ç¨‹å¼ä¸­å‘¼å« log API logger.info(\u0026quot;message\u0026quot;) Appender æ±ºå®šæ—¥èªŒè¼¸å‡ºåˆ°å“ªè£¡ Console / File / JSON Layout æ§åˆ¶æ—¥èªŒæ ¼å¼ %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1} - %m%n Configuration æ§åˆ¶ Loggerã€Appenderã€ç­–ç•¥ log4j2.properties å¸¸è¦‹ Log Levelï¼š\n1 TRACE \u0026lt; DEBUG \u0026lt; INFO \u0026lt; WARN \u0026lt; ERROR \u0026lt; FATAL å»ºè­°ï¼š Prod â†’ rootLogger = WARN Dev â†’ rootLogger = INFO Log4j2 è¨­å®š /etc/spark/log4j2.properties\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 status = WARN name = SparkLog4j2 property.logDir = ${sys:spark.yarn.app.container.log.dir:-/var/log/spark} property.logName = ${sys:logfile.name:-spark-app} property.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %c{1} - %msg%n # Console Appenderï¼ˆDev å¯ç”¨ï¼‰ appender.console.type = Console appender.console.name = CONSOLE appender.console.layout.type = PatternLayout appender.console.layout.pattern = ${pattern} # Rolling File Appender appender.rolling.type = RollingFile appender.rolling.name = ROLLING appender.rolling.fileName = ${logDir}/${logName}.log appender.rolling.filePattern = ${logDir}/${logName}.%d{yyyy-MM-dd}.%i.log.gz appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = ${pattern} appender.rolling.policies.type = Policies appender.rolling.policies.time.type = TimeBasedTriggeringPolicy appender.rolling.policies.time.interval = 1 appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size = 512MB appender.rolling.strategy.type = DefaultRolloverStrategy appender.rolling.strategy.max = 7 # Async Appenderï¼ˆå»ºè­° Prod é–‹å•Ÿï¼‰ appender.asyncText.type = Async appender.asyncText.name = ASYNC_TEXT appender.asyncText.appenderRef.ref = ROLLING # Root Logger rootLogger.level = WARN rootLogger.appenderRefs = consoleRef, rollingRef rootLogger.appenderRef.consoleRef.ref = CONSOLE rootLogger.appenderRef.rollingRef.ref = ASYNC_TEXT # Parquetã€Jettyã€Hive logger.parquet.name = org.apache.parquet logger.parquet.level = ERROR logger.jetty.name = org.spark_project.jetty logger.jetty.level = WARN logger.hive.name = org.apache.hadoop.hive.metastore.RetryingHMSHandler logger.hive.level = FATAL # Custom Application logger logger.app.name = com.example.spark logger.app.level = INFO logger.app.additivity = false logger.app.appenderRefs = appConsole, appFile logger.app.appenderRef.appConsole.ref = CONSOLE logger.app.appenderRef.appFile.ref = ASYNC_TEXT YARN ç’°å¢ƒé›†ä¸­æ—¥èªŒæ–¹æ¡ˆ åœ¨ Spark on YARN æ¨¡å¼ä¸‹ï¼ŒDriver / Executor åˆ†æ•£åœ¨ä¸åŒç¯€é»ï¼Œé è¨­æ—¥èªŒåˆ†æ•£æ–¼ï¼š\n1 2 /var/log/hadoop-yarn/container/\u0026lt;app_id\u0026gt;/stdout /var/log/hadoop-yarn/container/\u0026lt;app_id\u0026gt;/stderr æ–¹æ¡ˆ 1ï¼šYARN Log Aggregationï¼ˆæœ€ç°¡å–®ï¼‰ é©åˆä¸­å°å‹å¢é›†ï¼Œç„¡éœ€é¡å¤–å®‰è£ã€‚\nè¨­å®šæ­¥é©Ÿ åœ¨ yarn-site.xml å•Ÿç”¨ï¼š\n1 2 3 4 5 6 7 8 9 10 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.log-aggregation-enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- ä¿ç•™ 7 å¤©æ—¥èªŒ --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.log-aggregation.retain-seconds\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;604800\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; æŸ¥è©¢æ—¥èªŒ 1 yarn logs -applicationId \u0026lt;app_id\u0026gt; å„ªé» YARN åŸç”ŸåŠŸèƒ½ï¼Œç„¡éœ€é¡å¤–å…ƒä»¶ã€‚ Driver / Executor æ—¥èªŒæœƒé›†ä¸­åˆ° HDFSã€‚ é©åˆä»»å‹™çµæŸå¾ŒæŸ¥çœ‹å®Œæ•´æ—¥èªŒã€‚ ç¼ºé» æŸ¥è©¢éœ€é€é CLIï¼Œç„¡å³æ™‚ç›£æ§ã€‚ ç„¡å…¨æ–‡æª¢ç´¢ï¼Œåƒ…é©åˆå–®æ¬¡æ’éŒ¯ã€‚ æ–¹æ¡ˆ 2ï¼šLog4j2 Rolling + NFSï¼ˆå…±äº«æª”æ¡ˆç³»çµ±ï¼‰ é©åˆå·²æœ‰ NAS / NFS ï¼Œä¸¦å¸Œæœ›å³æ™‚é›†ä¸­æ—¥èªŒã€‚\nè¨­å®šæ­¥é©Ÿ è¨­å®š Log4j2 è¼¸å‡ºç›®éŒ„ 1 2 3 property.logDir = /mnt/shared-logs/spark property.logName = ${sys:logfile.name:-spark-app} appender.rolling.fileName = ${logDir}/${logName}.log spark-submit æŒ‡å®šä¸åŒæª”å 1 spark-submit --master yarn --conf \u0026#34;spark.driver.extraJavaOptions=-Dlogfile.name=myjob-driver\u0026#34; --conf \u0026#34;spark.executor.extraJavaOptions=-Dlogfile.name=myjob-exec\u0026#34; å„ªé» æ‰€æœ‰æ—¥èªŒé›†ä¸­åˆ° /mnt/shared-logs/sparkã€‚ å¯ç›´æ¥ tail -fã€grep å³æ™‚æŸ¥ Driver / Executor logã€‚ æˆæœ¬ä½ï¼Œéƒ¨ç½²ç°¡å–®ã€‚ ç¼ºé» éœ€è¦å…±äº«æª”æ¡ˆç³»çµ±ã€‚ éè¼‰æ™‚å¯èƒ½å½±éŸ¿ Executor å¯«å…¥æ•ˆèƒ½ã€‚ è«‹ç¢ºä¿ /mnt/shared-logs/spark æœ‰æ­£ç¢ºçš„è®€å¯«æ¬Šé™ï¼Œå¦å‰‡ Executor å¯èƒ½ç„¡æ³•å¯«å…¥æ—¥èªŒã€‚\nSpark on YARN + Log4j2 Logging ç¸½çµèˆ‡å»ºè­° Spark Prod ç’°å¢ƒè«‹å‡ç´šåˆ° Log4j2 (â‰¥ 2.17)ã€‚ çµ±ä¸€è·¯å¾‘ /var/log/spark æˆ– /mnt/shared-logs/spark â†’ Driverã€Executor æ—¥èªŒçµ±ä¸€ç®¡ç†ã€‚ ä½é ç®—æœ€ä½³æ–¹æ¡ˆï¼š YARN Log Aggregation â†’ é›†ä¸­åˆ° HDFSã€‚ Log4j2 Rolling + NFS â†’ å³æ™‚æŸ¥çœ‹æ—¥èªŒã€‚ å…©ç¨®æ–¹æ¡ˆå¯ åŒæ™‚å•Ÿç”¨ï¼Œå…¼é¡§å³æ™‚ç›£æ§èˆ‡æ—¥èªŒæ­¸æª”ã€‚ Reference Apache Spark å®˜æ–¹ https://spark.apache.org/docs/latest/ Spark Logging https://spark.apache.org/docs/latest/configuration.html#spark-logging Log4j2 \u0026amp; Config https://logging.apache.org/log4j/2.x/manual/configuration.html Spark on YARN https://spark.apache.org/docs/latest/running-on-yarn.html æœ€è¿‘æ‰“æ‹³è¢«æåˆ°è…¦è¢‹æœ‰é»ä¸éˆå…‰ ğŸ¤•\nå¯«æ–‡ç« éœ€è¦å’–å•¡ä¾†è£œè¡€ â˜•\nå¦‚æœä½ å–œæ­¡é€™ç¯‡å…§å®¹ï¼Œæ­¡è¿è«‹æˆ‘å–æ¯å’–å•¡ï¼\nLately Iâ€™ve been punched a bit too much in boxing ğŸ¥Š\nMy brain runs on coffee patches â˜•\nIf you enjoyed this post, fuel me with a cup!\nğŸ‘‰ Buy Me a Coffee\n","date":"2025-09-13T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_spark_log4j/","title":"PySpark å­¸ç¿’ç­†è¨˜ - Apache Spark èˆ‡ Log4j"},{"content":"SQL Server APPLY æŸ¥è©¢ è³‡æ–™è¡¨è³‡è¨Š (SQL Server) ç‚ºäº†è®“ç¯„ä¾‹å¯ä»¥ç›´æ¥åŸ·è¡Œï¼Œæˆ‘å€‘å…ˆå»ºç«‹å…©å¼µè¡¨ï¼š\nCustomers è¡¨ æ¬„ä½åç¨± å‹åˆ¥ èªªæ˜ customer_id INT (PK) å®¢æˆ¶ç·¨è™Ÿ name VARCHAR(20) å®¢æˆ¶åç¨± Orders è¡¨ æ¬„ä½åç¨± å‹åˆ¥ èªªæ˜ order_id INT (PK) è¨‚å–®ç·¨è™Ÿ customer_id INT (FK) å®¢æˆ¶ç·¨è™Ÿ order_date DATETIME è¨‚å–®æ—¥æœŸ amount DECIMAL(10,2) è¨‚å–®é‡‘é¡ å»ºç«‹è³‡æ–™è¡¨ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 IF OBJECT_ID(\u0026#39;Orders\u0026#39;, \u0026#39;U\u0026#39;) IS NOT NULL DROP TABLE Orders; IF OBJECT_ID(\u0026#39;Customers\u0026#39;, \u0026#39;U\u0026#39;) IS NOT NULL DROP TABLE Customers; CREATE TABLE Customers ( customer_id INT PRIMARY KEY, name VARCHAR(100) ); CREATE TABLE Orders ( order_id INT PRIMARY KEY, customer_id INT NOT NULL FOREIGN KEY REFERENCES Customers(customer_id), order_date DATETIME NOT NULL, amount DECIMAL(10,2) NOT NULL ); æ’å…¥æ¸¬è©¦è³‡æ–™ 1 2 3 4 5 6 7 8 9 10 11 INSERT INTO Customers(customer_id, name) VALUES (1, \u0026#39;Alice\u0026#39;), (2, \u0026#39;Bob\u0026#39;), (3, \u0026#39;Carol\u0026#39;); INSERT INTO Orders(order_id, customer_id, order_date, amount) VALUES (101, 1, \u0026#39;2025-08-01\u0026#39;, 120.00), (102, 1, \u0026#39;2025-08-10\u0026#39;, 90.00), (103, 1, \u0026#39;2025-08-15\u0026#39;, 250.00), (201, 2, \u0026#39;2025-07-20\u0026#39;, 300.00), (202, 2, \u0026#39;2025-08-05\u0026#39;, 80.00); APPLY åŸç† åœ¨ SQL Server ä¸­ï¼ŒAPPLY æ˜¯ä¸€ç¨® æ©«å‘é—œè¯ (lateral join) çš„æ©Ÿåˆ¶ã€‚\nCROSS APPLY åƒ INNER JOIN OUTER APPLY åƒ LEFT JOIN PostgreSQL å°æ‡‰ï¼šLATERAL\nCROSS APPLY vs OUTER APPLY ç‰¹æ€§ CROSS APPLY OUTER APPLY é¡ä¼¼æ–¼ INNER JOIN LEFT JOIN å¦‚æœå³é‚Šæ²’æœ‰è³‡æ–™ ä¸å›å‚³è©²åˆ— ä¿ç•™å·¦é‚Šåˆ—ï¼Œå³é‚Šæ¬„ä½å¡« NULL é©ç”¨å ´æ™¯ åªè¦å³é‚Šæœ‰çµæœ å³ä½¿å³é‚Šæ²’æœ‰çµæœä¹Ÿè¦é¡¯ç¤º ç¯„ä¾‹ 1ï¼šæ¯ä½å®¢æˆ¶å–æœ€æ–°ä¸€ç­†è¨‚å–® CROSS APPLY 1 2 3 4 5 6 7 8 SELECT c.customer_id, c.name, o1.order_id, o1.order_date, o1.amount FROM Customers AS c CROSS APPLY ( SELECT TOP (1) o.* FROM Orders AS o WHERE o.customer_id = c.customer_id ORDER BY o.order_date DESC ) AS o1; OUTER APPLY 1 2 3 4 5 6 7 8 SELECT c.customer_id, c.name, o1.order_id, o1.order_date, o1.amount FROM Customers AS c OUTER APPLY ( SELECT TOP (1) o.* FROM Orders AS o WHERE o.customer_id = c.customer_id ORDER BY o.order_date DESC ) AS o1; é æœŸçµæœæ¯”è¼ƒ CROSS APPLY â†’ åªé¡¯ç¤ºæœ‰è¨‚å–®çš„å®¢æˆ¶\ncustomer_id name order_id order_date amount 1 Alice 103 2025-08-15 250.00 2 Bob 202 2025-08-05 80.00 OUTER APPLY â†’ ä¿ç•™æ‰€æœ‰å®¢æˆ¶\ncustomer_id name order_id order_date amount 1 Alice 103 2025-08-15 250.00 2 Bob 202 2025-08-05 80.00 3 Carol NULL NULL NULL ç¯„ä¾‹ 2ï¼šæ¯ä½å®¢æˆ¶å–é‡‘é¡æœ€é«˜å‰ä¸‰ç­†è¨‚å–® 1 2 3 4 5 6 7 8 9 SELECT c.customer_id, c.name, d.order_id, d.order_date, d.amount FROM Customers AS c OUTER APPLY ( SELECT TOP (3) o.* FROM Orders o WHERE o.customer_id = c.customer_id ORDER BY o.amount DESC ) d ORDER BY c.customer_id, d.amount DESC NULLS LAST; é æœŸçµæœ\ncustomer_id name order_id order_date amount 1 Alice 103 2025-08-15 250.00 1 Alice 101 2025-08-01 120.00 1 Alice 102 2025-08-10 90.00 2 Bob 201 2025-07-20 300.00 2 Bob 202 2025-08-05 80.00 3 Carol NULL NULL NULL æ•ˆèƒ½æ³¨æ„äº‹é … APPLY æœƒé‡å°å·¦è¡¨æ¯åˆ—åŸ·è¡Œä¸€æ¬¡å­æŸ¥è©¢ å»ºè­°å»ºç«‹è¤‡åˆç´¢å¼•ï¼š 1 2 CREATE INDEX idx_orders_customer_date ON Orders(customer_id, order_date DESC); CROSS APPLY ç¨å¿«æ–¼ OUTER APPLY Reference ç†è§£ SQL Server çš„ CROSS APPLY å’Œ OUTER APPLY æŸ»è©¢\næœ€è¿‘æ‰“æ‹³è¢«æåˆ°è…¦è¢‹æœ‰é»ä¸éˆå…‰ ğŸ¤•\nå¯«æ–‡ç« éœ€è¦å’–å•¡ä¾†è£œè¡€ â˜•\nå¦‚æœä½ å–œæ­¡é€™ç¯‡å…§å®¹ï¼Œæ­¡è¿è«‹æˆ‘å–æ¯å’–å•¡ï¼\nLately Iâ€™ve been punched a bit too much in boxing ğŸ¥Š\nMy brain runs on coffee patches â˜•\nIf you enjoyed this post, fuel me with a cup!\nğŸ‘‰ Buy Me a Coffee\n","date":"2025-09-06T00:00:00Z","permalink":"https://www.noleonardblog.com/p/sql_server_apply_1/","title":"SQL SERVER `APPLY` æŸ¥è©¢"},{"content":"ğŸ”¥ PySpark - Basic Exec Model \u0026amp; Resource (ä»¥ spark.master=local[3] ç‚ºä¾‹)\næœ¬ç­†è¨˜æ¶µè“‹ä»¥ä¸‹å…§å®¹ï¼š\nSpark ç¨‹å¼åŸ·è¡Œæ–¹å¼ Spark é‹ç®—æ¶æ§‹èˆ‡æäº¤æµç¨‹ Spark åŸ·è¡Œæ¨¡å¼èˆ‡ Cluster Manager Local æ¨¡å¼ç¯„ä¾‹ åŸºæœ¬è³‡æºèª¿æ•´å»ºè­° Spark ç¨‹å¼åŸ·è¡Œæ–¹å¼ Spark æä¾›å…©å¤§é¡å‹çš„åŸ·è¡Œæ–¹å¼ï¼šäº’å‹•å¼é–‹ç™¼èˆ‡æäº¤æ‰¹æ¬¡ä»»å‹™ã€‚\näº’å‹•å¼é–‹ç™¼ (Interactive Clients) ğŸ§ª é©åˆé–‹ç™¼èˆ‡è³‡æ–™æ¢ç´¢ï¼Œå¿«é€Ÿæ¸¬è©¦ç¨‹å¼èˆ‡é©—è­‰é‚è¼¯ã€‚\nå·¥å…· åŠŸèƒ½ é©ç”¨å ´æ™¯ spark-shell Scala / Python / R REPLï¼Œå¿«é€Ÿæ¸¬è©¦ å°å‹æ¸¬è©¦ã€å­¸ç¿’ Notebook Jupyterã€Zeppelinã€Databricks Notebook è³‡æ–™æ¢ç´¢ã€å¯è¦–åŒ–åˆ†æ ç‰¹é»ï¼šå¿«é€Ÿé©—è­‰é‚è¼¯ï¼Œä½†ä¸é©åˆé•·æ™‚é–“é‹è¡Œæˆ–å¤§è¦æ¨¡è¨ˆç®—ã€‚\næäº¤æ‰¹æ¬¡ä»»å‹™ (Submit Job) ğŸš€ é©åˆæ­£å¼ç’°å¢ƒï¼Œå°‡ Spark Job æäº¤çµ¦å¢é›†é‹è¡Œã€‚\nå·¥å…· åŠŸèƒ½ é©ç”¨å ´æ™¯ spark-submit æœ€å¸¸ç”¨æ–¹å¼ï¼Œæäº¤ Application è‡³å¢é›† Prod ETLã€æ‰¹æ¬¡è™•ç† Databricks é›²ç«¯ Notebook å¹³å°ï¼Œå…§å»º Spark é‹è¡Œç’°å¢ƒ é›²ç«¯æ•¸æ“šè™•ç† REST API / Web UI æäº¤ã€ç›£æ§ã€ç®¡ç† Spark Job è‡ªå‹•åŒ–èª¿åº¦ Spark é‹ç®—æ¶æ§‹èˆ‡æäº¤æµç¨‹ Spark æ¡ç”¨ Driver + Executor æ¶æ§‹ï¼Œé€é Cluster Manager ç®¡ç†è³‡æºã€‚\næ ¸å¿ƒå…ƒä»¶ å…ƒä»¶ é¡å‹ åŠŸèƒ½ Client æäº¤ç«¯ æäº¤ Jobï¼Œä¾‹å¦‚ spark-submit Driver JVM Process ä»»å‹™èª¿åº¦ä¸­å¿ƒï¼Œè² è²¬ Stage åˆ†å‰²èˆ‡ Task åˆ†é… Executor JVM Process åŸ·è¡Œ Tasksï¼Œè² è²¬è¨ˆç®—è³‡æ–™ Task Thread Executor å…§åŸ·è¡Œçš„æœ€å°è¨ˆç®—å–®ä½ Cluster Manager è³‡æºç®¡ç†å™¨ åˆ†é…å¢é›† CPU / Memory è³‡æºï¼Œå•Ÿå‹• Executors Spark Job æäº¤æµç¨‹ Spark åŸ·è¡Œæ¨¡å¼èˆ‡ Cluster Manager Spark æ”¯æ´å¤šç¨®åŸ·è¡Œæ¨¡å¼ï¼Œæ±ºå®š Driver èˆ‡ Executor çš„é‹è¡Œä½ç½®ã€‚\næ¨¡å¼ spark.master è¨­å®š JVM Process æ•¸é‡ Thread æ•¸é‡ é©ç”¨å ´æ™¯ Local[3] local[3] 1 Driver + 1 Executor 3 æœ¬æ©Ÿæ¸¬è©¦ / æ¨¡æ“¬ä¸¦è¡Œ Local[*] local[*] 1 Driver + 1 Executor CPUæ ¸å¿ƒæ•¸ å£“æ¸¬æˆ–å–®æ©Ÿæ¥µé™ Standalone spark://host:7077 å¤š Executors å¤š Threads Spark åŸç”Ÿå¢é›† YARN yarn Container æ±ºå®š å¤š Threads Hadoop ç”Ÿæ…‹ Kubernetes k8s:// Pod æ±ºå®š å¤š Threads é›²ç«¯åŸç”Ÿ Mesos mesos:// å¤š Executors å¤š Threads å¤§å‹ä¼æ¥­å…±äº«å¢é›† Local æ¨¡å¼ç¯„ä¾‹ï¼šspark.master=local[3] local[3] é‹è¡Œæ¶æ§‹åœ– 1 2 3 - 1 Driver + 1 Executor JVM - Executor å…§ 3 Threads â†’ åŒæ™‚è™•ç† 3 Tasks - è‹¥ 12 Partitions â†’ Spark éœ€åˆ† 4 è¼ªåŸ·è¡Œ YARN æ¨¡å¼æ¶æ§‹åœ– Kubernetes æ¨¡å¼æ¶æ§‹åœ– Spark å¯å˜—è©¦è³‡æºé…ç½®ç­–ç•¥ æ¨¡å¼ Driver ä½ç½® Executor JVM æ•¸ æ¯ Executor Threads æœ€å¤§ä½µè¡Œåº¦ é©ç”¨å ´æ™¯ local[3] æœ¬æ©Ÿ 1 3 3 å°å‹æ¸¬è©¦ YARN ResourceManager 4 4 16 Hadoop ç”Ÿæ…‹ K8s Pod 4 4 16 é›²ç«¯åŸç”Ÿ æœ€å¤§ä½µè¡Œåº¦å…¬å¼ï¼š\n1 Max Concurrent Tasks = Executors Ã— Executor Cores Spark ä¸€äº›èª¿æ•´å»ºè­° Partition å»ºè­°å¤§å° â‰ˆ 128MB Executors Ã— Cores â‰ˆ Partition æ•¸ / 2~3 Shuffle Partition = Executors Ã— Cores Ã— 2 é¿å…å–® Executor éå¤š Threads â†’ é™ä½ GC è² æ“” Production å»ºè­°é–‹å•Ÿå‹•æ…‹è³‡æºé…ç½®ï¼š 1 --conf spark.dynamicAllocation.enabled=true ç¸½çµ local[3] â†’ 1 Executor JVM + 3 Threads â†’ é©åˆé–‹ç™¼èˆ‡æ¨¡æ“¬ä¸¦è¡Œ Production â†’ å»ºè­°ä½¿ç”¨ YARN / K8s / Standalone Spark æ•ˆèƒ½èª¿å„ªæ ¸å¿ƒä¸‰æ­¥ï¼š æ±ºå®š Partition æ•¸é‡ è¨­å®š Executors Ã— Cores èª¿æ•´ Shuffle Partitions Reference PySpark - Apache Spark Programming in Python for beginners\næœ€è¿‘æ‰“æ‹³è¢«æåˆ°è…¦è¢‹æœ‰é»ä¸éˆå…‰ ğŸ¤•\nå¯«æ–‡ç« éœ€è¦å’–å•¡ä¾†è£œè¡€ â˜•\nå¦‚æœä½ å–œæ­¡é€™ç¯‡å…§å®¹ï¼Œæ­¡è¿è«‹æˆ‘å–æ¯å’–å•¡ï¼\nLately Iâ€™ve been punched a bit too much in boxing ğŸ¥Š\nMy brain runs on coffee patches â˜•\nIf you enjoyed this post, fuel me with a cup!\nğŸ‘‰ Buy Me a Coffee\n","date":"2025-08-29T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_basic_exec_model/","title":"PySpark å­¸ç¿’ç­†è¨˜ - Basic Exec Model \u0026 Resource"},{"content":"ğŸ”¥ PySpark - Getting Start PySpark å­¸ç¿’\næœ¬ç­†è¨˜æ¶µè“‹ä»¥ä¸‹å…§å®¹ï¼š\nå»ºç«‹ Spark Session è®€å– CSV åŸå§‹è³‡æ–™ æ¬„ä½æ¨™æº–åŒ–ï¼ˆæ‰¹æ¬¡é‡æ–°å‘½åï¼‰ å»ºç«‹æš«å­˜è¡¨ä¾› SQL æŸ¥è©¢ æŸ¥è©¢èˆ‡è½‰æ›ç¯„ä¾‹ï¼šé€±æ¬¡çµ±è¨ˆ Spark å®˜æ–¹æ–‡ä»¶æ˜¯ä½ å€‹å¥½å¤¥ä¼´ spark.apache.org\nDataFrame Methods\nActions Are DataFrame Operations that kick off a Spark Job execution and return to the Spark Driver collect count describe first foreach foreachPartition head show summary tail take toLocalIterator Transformations Spark DataFrame transformation produces a newly transformed Dataframe agg alias coalesce colRegex crossJoin crosstab cube distinct drop drop_duplicates dropDuplicates dropna exceptAll filter groupby \u0026hellip; Functions/Methods Dataframe Methods or function which are not categorized into Actions or Transformations approxQuantile cache checkpoint createGlobalTempView createOrReplaceGlobalTempView createOrReplaceTempView createTempView explain hint inputFiles isLocal localCheckpoint toDF toJSON \u0026hellip; Dataset - Fire Department\nğŸ“¦ 1. å»ºç«‹ Spark Session èˆ‡è®€å–è³‡æ–™ 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyspark.sql import SparkSession from pyspark.sql.functions import * spark = SparkSession.builder \\ .appName(\u0026#34;basic\u0026#34;) \\ .master(\u0026#34;local[2]\u0026#34;) \\ .getOrCreate() fire_df = spark.read \\ .format(\u0026#34;csv\u0026#34;) \\ .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) \\ .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) \\ .load(\u0026#34;data/sf_file_calls.csv\u0026#34;) ğŸ› ï¸ 2. æ¬„ä½é‡æ–°å‘½åï¼ˆæ¨™æº–åŒ–ï¼‰ æœ‰äº›æ¬„ä½åŒ…å«ç©ºæ ¼æˆ–ä¸é©åˆç¨‹å¼ä½¿ç”¨çš„å‘½åï¼Œé€éå­—å…¸æ˜ å°„çµ±ä¸€åç¨±ï¼š\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 rename_map = { \u0026#34;Call Number\u0026#34;: \u0026#34;CallNumber\u0026#34;, \u0026#34;Unit ID\u0026#34;: \u0026#34;UnitID\u0026#34;, \u0026#34;Incident Number\u0026#34;: \u0026#34;IncidentNumber\u0026#34;, \u0026#34;Call Type\u0026#34;: \u0026#34;CallType\u0026#34;, \u0026#34;Call Date\u0026#34;: \u0026#34;CallDate\u0026#34;, \u0026#34;Watch Date\u0026#34;: \u0026#34;WatchDate\u0026#34;, \u0026#34;Received DtTm\u0026#34;: \u0026#34;ReceivedDtTm\u0026#34;, \u0026#34;Entry DtTm\u0026#34;: \u0026#34;EntryDtTm\u0026#34;, \u0026#34;Dispatch DtTm\u0026#34;: \u0026#34;DispatchDtTm\u0026#34;, \u0026#34;Response DtTm\u0026#34;: \u0026#34;ResponseDtTm\u0026#34;, \u0026#34;On Scene DtTm\u0026#34;: \u0026#34;OnSceneDtTm\u0026#34;, \u0026#34;Transport DtTm\u0026#34;: \u0026#34;TransportDtTm\u0026#34;, \u0026#34;Hospital DtTm\u0026#34;: \u0026#34;HospitalDtTm\u0026#34;, \u0026#34;Call Final Disposition\u0026#34;: \u0026#34;CallFinalDisposition\u0026#34;, \u0026#34;Available DtTm\u0026#34;: \u0026#34;AvailableDtTm\u0026#34;, \u0026#34;Zipcode of Incident\u0026#34;: \u0026#34;ZipcodeofIncident\u0026#34;, \u0026#34;Station Area\u0026#34;: \u0026#34;StationArea\u0026#34;, \u0026#34;Original Priority\u0026#34;: \u0026#34;OriginalPriority\u0026#34;, \u0026#34;FinalPriority\u0026#34;: \u0026#34;FinalPriority\u0026#34;, \u0026#34;ALS Unit\u0026#34;: \u0026#34;ALSUnit\u0026#34;, \u0026#34;Call Type Group\u0026#34;: \u0026#34;CallTypeGroup\u0026#34;, \u0026#34;Number of Alarms\u0026#34;: \u0026#34;NumberofAlarms\u0026#34;, \u0026#34;Unit Type\u0026#34;: \u0026#34;UnitType\u0026#34;, \u0026#34;Unit sequence in call dispatch\u0026#34;: \u0026#34;Unitsequenceincalldispatch\u0026#34;, \u0026#34;Fire Prevention District\u0026#34;: \u0026#34;FirePreventionDistrict\u0026#34;, \u0026#34;Supervisor District\u0026#34;: \u0026#34;SupervisorDistrict\u0026#34;, \u0026#34;Neighborhooods - Analysis Boundaries\u0026#34;: \u0026#34;neighborhoods_analysis_boundaries\u0026#34; } for old, new in rename_map.items(): fire_df = fire_df.withColumnRenamed(old, new) ğŸ§ª 3. å»ºç«‹æš«å­˜è¡¨ä¾› SQL æŸ¥è©¢ 1 fire_df.createOrReplaceTempView(\u0026#34;fire_calls\u0026#34;) ä½ å¯ä»¥åœ¨ Spark SQL ä¸­æŸ¥è©¢ï¼š\n1 spark.sql(SELECT * FROM fire_calls LIMIT 5).show() ğŸ§¾ 4. è³‡æ–™çµæ§‹èˆ‡é è¦½ 1 2 fire_df.printSchema() fire_df.show(2, truncate=False, vertical=True) ğŸ“Š 5. åˆ†æï¼š2018 å¹´æ¯é€±é€šå ±äº‹ä»¶æ•¸ï¼ˆé€±æ¬¡çµ±è¨ˆï¼‰ 1 2 3 4 5 6 7 8 9 10 11 12 from pyspark.sql.functions import to_date, year, weekofyear, col result = spark.table(\u0026#34;fire_calls\u0026#34;) \\ .withColumn(\u0026#34;call_date\u0026#34;, to_date(col(\u0026#34;CallDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)) \\ .filter(year(col(\u0026#34;call_date\u0026#34;)) == 2018) \\ .withColumn(\u0026#34;week_of_year\u0026#34;, weekofyear(col(\u0026#34;call_date\u0026#34;))) \\ .groupBy(\u0026#34;week_of_year\u0026#34;) \\ .count() \\ .select(\u0026#34;week_of_year\u0026#34;, \u0026#34;count\u0026#34;) \\ .orderBy(col(\u0026#34;count\u0026#34;).desc()) result.show() ğŸ“˜ ç­†è¨˜ï¼š to_date(...)ï¼šè½‰æ›å­—ä¸²æ—¥æœŸæ ¼å¼ year(...)ï¼šæ“·å–å¹´ä»½ weekofyear(...)ï¼šæ“·å–é€±æ¬¡ï¼ˆ1~52ï¼‰ groupBy(...).count()ï¼šçµ±è¨ˆæ¯é€±é€šå ±æ•¸ orderBy(...desc())ï¼šä¾æ•¸é‡æ’åº âœ… å»¶ä¼¸å»ºè­° è‹¥è¦è¼¸å‡ºç‚º CSVï¼š\n1 result.write.csv(\u0026#34;output/fire_calls_by_week.csv\u0026#34;, header=True) è‹¥è¦ç•«å‡ºè¶¨å‹¢åœ–ï¼Œå¯ç”¨ï¼š\n1 result.toPandas().plot(x=\u0026#34;week_of_year\u0026#34;, y=\u0026#34;count\u0026#34;, kind=\u0026#34;bar\u0026#34;) Reference PySpark - Apache Spark Programming in Python for beginners\næœ€è¿‘æ‰“æ‹³è¢«æåˆ°è…¦è¢‹æœ‰é»ä¸éˆå…‰ ğŸ¤•\nå¯«æ–‡ç« éœ€è¦å’–å•¡ä¾†è£œè¡€ â˜•\nå¦‚æœä½ å–œæ­¡é€™ç¯‡å…§å®¹ï¼Œæ­¡è¿è«‹æˆ‘å–æ¯å’–å•¡ï¼\nLately Iâ€™ve been punched a bit too much in boxing ğŸ¥Š\nMy brain runs on coffee patches â˜•\nIf you enjoyed this post, fuel me with a cup!\nğŸ‘‰ Buy Me a Coffee\n","date":"2025-08-23T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_getting_start/","title":"PySpark å­¸ç¿’ç­†è¨˜ - Getting Start"},{"content":"å¾æˆ‘çš„ Medium æ–‡ç« è½‰ç™¼: BigQuery Logging \u0026amp; Monitor Google Cloud Loggingã€Monitoring å’Œ Alerting èƒ½ç¢ºä¿ BigQuery å·¥ä½œè² è¼‰å¯é é‹è¡Œã€å„ªåŒ–æ€§èƒ½å’Œæ§åˆ¶æˆæœ¬çš„é—œéµå·¥å…·ã€‚\næ ¹æ“š Google Cloud æ–‡ä»¶ä»¥ä¸‹å¯ä»¥è®“ç›£æ§å„ªåŒ–æ›´å¤šæ‡‰ç”¨é¢å‘ã€‚\nCloud Monitoring æŸ¥çœ‹ BigQuery æŒ‡æ¨™ INFORMATION_SCHEMA views æŸ¥çœ‹é—œæ–¼ jobs, datasets, tables, reservations çš„ metadata Audit Logs æŸ¥çœ‹å„ç¨® Event åƒæ˜¯ (create/delete a table, purchase slots, run jobs)\nCloud Monitoring å…·å‚™ä»¥ä¸‹çš„åŠŸèƒ½:\nå»ºç«‹å„€è¡¨æ¿ã€è¦–è¦ºåŒ–åœ–è¡¨ ç•°å¸¸çš„æŸ¥è©¢çš„å‘Šè­¦ (ex: ä½¿ç”¨è€…æŸ¥è©¢é‡éå¤§) åˆ†ææ­·å²æ•ˆèƒ½ã€åˆ†ææŸ¥è©¢é‡ Dashboard åŸºæœ¬ç¯„ä¾‹ Dashboard Page Table å¤§å°ç¸½è¦½ Dataset å¤§å°ç¸½è¦½ Cloud Logging Cloud Audit Logs ç®¡ç†å„ç¨®äº‹ä»¶ï¼Œåƒæ˜¯ Table çš„ CRUDã€‚ ä½¿ç”¨è€…æŸ¥è©¢å„²å­˜çš„æ•¸æ“šã€‚ ç³»çµ±ç”Ÿæˆçš„ç³»çµ±äº‹ä»¶ã€‚ Log Viewer LogEntry Objects å¯ä»¥é€éæ’ˆå– Logging è³‡è¨Šä½œç‚ºä¸»å‹•å‘Šè­¦ GCP Service Account ä½¿ç”¨ BigQuery æŸ¥è©¢é‡æ’ˆå–ç¯„ä¾‹ 1 2 3 4 5 resource.type=\u0026#34;bigquery_resource\u0026#34; log_name=\u0026#34;projects/\u0026lt;project_id\u0026gt;/logs/cloudaudit.googleapis.com%2Fdata_access\u0026#34; protoPayload.methodName=\u0026#34;jobservice.jobcompleted\u0026#34; protoPayload.serviceData.jobCompletedEvent.job.jobStatistics.totalProcessedBytes \u0026gt; 75000000000 protoPayload.authenticationInfo.principalEmail=~\u0026#34;(iam\\.gserviceaccount\\.com)$\u0026#34; GCP Schedule Query æ’ˆå– Error ç¯„ä¾‹ 1 2 3 4 resource.type=\u0026#34;bigquery_resource\u0026#34; log_name=\u0026#34;projects/\u0026lt;project_id\u0026gt;/logs/cloudaudit.googleapis.com%2Fdata_access\u0026#34; protoPayload.serviceData.jobCompletedEvent.job.jobName.jobId=~\u0026#34;scheduled_query\u0026#34; severity=ERROR ä»¥ä¸Šæ’ˆå–å‡ºä¾†çš„å¯ä»¥å»åšå®¢è£½åŒ–æŒ‡æ¨™ä¸¦ä½¿ç”¨ Cloud Alert å‘Šè­¦åˆ° Slack æˆ–æ˜¯å…¶ä»–é€šè¨Šè»Ÿé«”ã€‚\nLog export to bigquery å¯ä»¥åŒ¯å‡ºè‡³ BigQuery åšè¢«å‹•ç›£æ§å ±è¡¨ï¼Œä¸¦ä¸”é‡å°æ­·å²è³‡æ–™åšåˆ†æã€‚\næœ‰æ™‚å€™ Log Viewer ä¸Šè¼ƒé›£ä»¥é–±è®€åŠä½¿ç”¨, å¯ä»¥é€é SQL çš„æ–¹å¼æŸ¥è©¢ã€‚ Audit log é è¨­ä¿ç•™ 30 å¤©ï¼Œå¯ä»¥å°‡ Log åŒ¯å‡ºä¸¦é‡å°æ­·å²è³‡æ–™åšæŸ¥è©¢ã€‚ Audit log åŒ¯å‡ºæ™‚è¨˜å¾—è¨­å®šéæ¿¾ï¼Œé¿å…é‡éå¤§ã€‚ Step:\nBQ å»ºç«‹è³‡æ–™é›† Logging å»ºç«‹æ¥æ”¶å™¨\nBigQuery Logging Table: Reference: Google Cloud OnBoard: é–‹å§‹å»ºæ§‹ä¼æ¥­è³‡æ–™å€‰å„²\nINFORMATION SCHEMA Intro\nBigQuery Audit Log\nBigQuery çš„ä¸‰ç¨®ç›£æ§æ–¹å¼ â€“ åŒ¯å‡º Cloud Logging\nâ€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€”\né€™æ˜¯ä¸€å€‹ç”± Gemini Advanced ç”¢ç”Ÿçš„ Summary\nLogging (æ—¥èªŒè¨˜éŒ„) è©³ç´°è¨˜éŒ„æ“ä½œæ­·å²ï¼š BigQuery æ—¥èªŒæœƒè©³ç´°è¨˜éŒ„å„ç¨®æ“ä½œï¼Œå¦‚æŸ¥è©¢åŸ·è¡Œã€è³‡æ–™è¼‰å…¥ã€è¡¨æ ¼å»ºç«‹ç­‰ã€‚é€™æœ‰åŠ©æ–¼è¿½è¹¤ä½œæ¥­æ­·å²ã€èª¿æŸ¥éŒ¯èª¤åŸå› ï¼Œä¸¦æ»¿è¶³åˆè¦å¯©è¨ˆéœ€æ±‚ã€‚ æ·±å…¥äº†è§£ç³»çµ±è¡Œç‚ºï¼š é€éåˆ†ææ—¥èªŒï¼Œæ‚¨å¯ä»¥æ·±å…¥äº†è§£ BigQuery çš„é‹è¡Œç‹€æ³ï¼Œè­˜åˆ¥æ½›åœ¨å•é¡Œï¼Œä¸¦å„ªåŒ–æŸ¥è©¢æ€§èƒ½ã€‚ é•·æœŸæ•¸æ“šå­˜å„²ï¼š æ—¥èªŒå¯ä»¥é•·æœŸå­˜å„²ï¼Œä¾›æœªä¾†åˆ†æå’Œå¯©è¨ˆä½¿ç”¨ã€‚\nMonitoring (ç›£æ§) å¯¦æ™‚ç³»çµ±ç‹€æ…‹å¯è¦–åŒ–ï¼š ç›£æ§æä¾› BigQuery é—œéµæŒ‡æ¨™çš„å¯¦æ™‚å¯è¦–åŒ–ï¼Œå¦‚æŸ¥è©¢åŸ·è¡Œæ™‚é–“ã€æ§½ä½ä½¿ç”¨ç‡ã€æŸ¥è©¢æˆæœ¬ç­‰ã€‚é€™æœ‰åŠ©æ–¼åŠæ™‚ç™¼ç¾æ€§èƒ½ç“¶é ¸å’Œç•°å¸¸æƒ…æ³ã€‚ è¶¨å‹¢åˆ†æèˆ‡å®¹é‡è¦åŠƒï¼š é€éåˆ†ææ­·å²ç›£æ§æ•¸æ“šï¼Œæ‚¨å¯ä»¥äº†è§£ BigQuery ä½¿ç”¨è¶¨å‹¢ï¼Œé æ¸¬æœªä¾†éœ€æ±‚ï¼Œä¸¦ç›¸æ‡‰åœ°è¦åŠƒå®¹é‡ã€‚ ä¸»å‹•å•é¡Œè­˜åˆ¥ï¼š ç›£æ§å¯ä»¥å¹«åŠ©æ‚¨åœ¨å•é¡Œå½±éŸ¿ç”¨æˆ¶ä¹‹å‰ä¸»å‹•è­˜åˆ¥ä¸¦è§£æ±ºã€‚\nAlerting (è­¦å ±) åŠæ™‚é€šçŸ¥ç•°å¸¸æƒ…æ³ï¼š ç•¶ BigQuery æŒ‡æ¨™è¶…éé è¨­é–¾å€¼æ™‚ï¼Œè­¦å ±æœƒåŠæ™‚é€šçŸ¥ç›¸é—œäººå“¡ï¼Œä»¥ä¾¿ç«‹å³æ¡å–è¡Œå‹•ã€‚ æˆæœ¬æ§åˆ¶ï¼š è­¦å ±å¯ä»¥å¹«åŠ©æ‚¨ç›£æ§æŸ¥è©¢æˆæœ¬ï¼Œé¿å…æ„å¤–çš„é«˜é¡è²»ç”¨ã€‚ ç¶œä¸Šæ‰€è¿°ï¼ŒBigQuery Loggingã€Monitoring å’Œ Alerting å°æ–¼ä»¥ä¸‹æ–¹é¢è‡³é—œé‡è¦ï¼š\nå„ªåŒ–æ€§èƒ½ï¼š è­˜åˆ¥æ€§èƒ½ç“¶é ¸ï¼Œå„ªåŒ–æŸ¥è©¢å’Œå·¥ä½œè² è¼‰ã€‚ æ§åˆ¶æˆæœ¬ï¼š ç›£æ§å’Œç®¡ç†æŸ¥è©¢æˆæœ¬ï¼Œé¿å…æ„å¤–è²»ç”¨ã€‚ æ»¿è¶³åˆè¦éœ€æ±‚ï¼š è¨˜éŒ„æ“ä½œæ­·å²ï¼Œæ»¿è¶³å¯©è¨ˆå’Œåˆè¦è¦æ±‚ã€‚ é€éæœ‰æ•ˆåˆ©ç”¨é€™äº›å·¥å…·ï¼Œæ‚¨å¯ä»¥å…¨é¢ç®¡ç† BigQuery ç’°å¢ƒï¼Œç¢ºä¿å…¶é«˜æ•ˆã€å¯é åœ°é‹è¡Œï¼Œä¸¦å……åˆ†ç™¼æ®å…¶å¼·å¤§çš„æ•¸æ“šåˆ†æèƒ½åŠ›ã€‚\n","date":"2024-11-04T00:00:00Z","image":"https://www.noleonardblog.com/p/bq-log-monitor/c2_hu_4f0b2f6ee2212dca.jpg","permalink":"https://www.noleonardblog.com/p/bq-log-monitor/","title":"BigQuery Logging \u0026 Monitoring"}]