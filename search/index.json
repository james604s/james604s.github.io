[{"content":"PySpark Data Lineage with Datahub 在我們日常的資料工作中，一個最令人頭痛的問題莫過於：「這張報表的資料到底從哪裡來？中間經過了哪些處理？」當資料流程變得越來越複雜時，如果沒有清晰的資料血緣 (Data Lineage)，追蹤問題、評估變更影響，甚至確保資料的可信度，都會變成一場噩夢。\n幸運的是，像 PySpark 這樣的強大處理引擎，搭配 Datahub 這樣的現代資料目錄 (Data Catalog)，可以幫助我們自動化地解開這個謎團。\n在 Datahub 中呈現的清晰資料血緣圖\nSpark \u0026amp; Lineage 在我們動手寫任何程式碼之前，最重要的一步是理解這背後的「魔法」是如何運作的。PySpark 本身並不會主動告訴外界它的資料流向，那麼 Datahub 是如何得知的呢？\n答案就在於 Spark 提供的一個強大機制：SparkListener 介面。\nSparkListener：安插在 Spark 內的事件監聽器 SparkListener 是 Spark 框架中基於觀察者模式 (Observer Pattern) 的一個實作。你可以把它想像成一個我們安插在 Spark 應用程式內部的「事件監聽器」。當你的 PySpark 腳本執行時，它內部的每一個重要動作——作業開始 (onJobStart)、作業結束 (onJobEnd)，乃至於一個查詢的執行 (onQueryExecution)——都會在 Spark 內部觸發一個對應的「事件」。\n我們的血緣擷取工具，正是透過實作這個介面來監聽這些事件。其中，對於資料血緣來說，最重要的事件是 onQueryExecution。\nLogical Plan：Spark 任務的原始設計藍圖 當 onQueryExecution 事件被觸發時，監聽器會收到一個 QueryExecution 物件，這個物件中包含了我們擷取血緣的關鍵資訊：邏輯計劃 (Logical Plan)。\nLogical Plan 是一個抽象語法樹 (Abstract Syntax Tree, AST)，它代表了你的 DataFrame 操作或 SQL 查詢在經過任何優化之前的完整結構。為什麼強調「未經優化」？因為它最忠實地反映了你編寫的程式碼邏輯。這個計畫詳細記錄了：\n資料來源 (Sources)：透過 UnresolvedRelation 等節點，標示資料是從哪個資料庫的哪張表讀取的。 轉換操作 (Transformations)：透過 Project (對應 select 或 withColumn)、Filter、Join 等節點，描述資料經過的每一個處理步驟。 資料去向 (Sink)：描述最終結果被寫入到哪個目的地。 血緣工具的核心任務，就是遞迴地遍歷這棵 Logical Plan 樹，解析出其中包含的輸入、輸出以及欄位級別的操作，從而完整地還原出資料的完整旅程。\n核心概念：無論是 Datahub 的原生整合工具，還是 OpenLineage 的開放標準工具，它們的核心都是實現了一個 SparkListener。它們的根本差異，不在於如何擷取 Logical Plan，而在於如何將解析後的血緣資訊「格式化」並「匯報」給 Datahub。\nETL Example (部分由 Gemini 生成) 理論講完了，讓我們來建立一個統一的實戰場景。以下是一個通用的 PySpark 腳本，它將是我們後續兩種方法的測試對象。\n任務目標：從 MSSQL 的 dbo.source_table 讀取資料，進行一系列轉換，然後將結果寫入 dbo.target_table。\nprocess_mssql_data.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from pyspark.sql import SparkSession from pyspark.sql.functions import col, lit, current_timestamp, upper, year, month def main(): \u0026#34;\u0026#34;\u0026#34; 一個從 MSSQL 讀取、轉換並寫入的 Spark ETL 作業。 \u0026#34;\u0026#34;\u0026#34; spark = SparkSession.builder.appName(\u0026#34;MSSQL_ETL_Lineage_Demo\u0026#34;).getOrCreate() # db conn jdbc_url = \u0026#34;jdbc:sqlserver://your_mssql_server:1433;databaseName=your_db\u0026#34; connection_properties = { \u0026#34;user\u0026#34;: \u0026#34;your_user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;your_password\u0026#34;, \u0026#34;driver\u0026#34;: \u0026#34;com.microsoft.sqlserver.jdbc.SQLServerDriver\u0026#34; } source_table = \u0026#34;dbo.source_table\u0026#34; target_table = \u0026#34;dbo.target_table\u0026#34; # Read Table print(\u0026#34;--- 正在從來源資料表讀取資料 ---\u0026#34;) source_df = spark.read.jdbc( url=jdbc_url, table=source_table, properties=connection_properties ) print(\u0026#34;來源 DataFrame Schema:\u0026#34;) source_df.printSchema() # Tranform print(\u0026#34;--- 正在進行資料轉換 ---\u0026#34;) transformed_df = source_df \\ .withColumn(\u0026#34;processed_at\u0026#34;, current_timestamp()) \\ .withColumn(\u0026#34;job_name\u0026#34;, lit(\u0026#34;MSSQL_ETL_Lineage_Demo\u0026#34;)) \\ .withColumn(\u0026#34;source_system_upper\u0026#34;, upper(col(\u0026#34;source_system\u0026#34;))) \\ .withColumn(\u0026#34;event_year\u0026#34;, year(col(\u0026#34;event_date\u0026#34;))) \\ .withColumn(\u0026#34;event_month\u0026#34;, month(col(\u0026#34;event_date\u0026#34;))) \\ .filter(col(\u0026#34;value\u0026#34;) \u0026gt; 100) print(\u0026#34;轉換後 DataFrame Schema:\u0026#34;) transformed_df.printSchema() # Write print(f\u0026#34;--- 正在將資料寫入 {target_table} ---\u0026#34;) transformed_df.write.jdbc( url=jdbc_url, table=target_table, mode=\u0026#34;overwrite\u0026#34;, properties=connection_properties ) print(\u0026#34;--- 作業成功完成！ ---\u0026#34;) spark.stop() if __name__ == \u0026#34;__main__\u0026#34;: main() 現在，我們將用兩種不同的 spark-submit 指令來執行同一個腳本，看看它們是如何實現血緣追蹤的。\n路徑 A：直達車 - Datahub 原生整合 (acryl-spark-lineage) 這條路徑像是搭乘一班為 Datahub 量身打造的直達專車，簡單、快速，改造 OpenLineage 標準以符合 Datahub 的格式。\n報告格式：將 Logical Plan 直接翻譯成 Datahub 內部能理解的格式，稱為 Metadata Change Proposals (MCPs)。 報告機制：透過 HTTP API 直接將這些 MCPs 發送給 Datahub 的核心服務 (GMS)。 確保你已經下載了 MSSQL JDBC 驅動\n1 export MSSQL_JDBC_JAR=\u0026#34;/path/to/mssql-jdbc.jar\u0026#34; spark-submit 指令：\n1 2 3 4 5 6 7 spark-submit \\ --master local[*] \\ --packages io.acryl:acryl-spark-lineage_2.12:0.2.18 \\ --jars ${MSSQL_JDBC_JAR} \\ --conf \u0026#34;spark.extraListeners=io.acryl.spark.AcrylSparkListener\u0026#34; \\ --conf \u0026#34;spark.datahub.url=http://your-datahub-gms-host:8080\u0026#34; \\ --conf \u0026#34;spark.datahub.token=YOUR_DATAHUB_API_TOKEN\u0026#34; \\ 1 process_mssql_data.py spark.extraListeners: 這是啟動魔法的關鍵，告訴 Spark 要掛載 Acryl 的監聽器。 spark.datahub.url: Datahub GMS 服務的端點 (Endpoint)。 路徑 B：自由行 - OpenLineage 開放標準 這條路徑採用了 OpenLineage 這個開放標準，提供了更大的靈活性和未來擴充性。\n報告格式：將 Logical Plan 翻譯成一種通用的、標準化的 OpenLineage Event (JSON 格式)。這份「報告」任何支援 OpenLineage 的平台都看得懂。 報告機制：將標準化的 JSON 事件發送到一個指定的 HTTP 端點。Datahub 恰好提供了一個這樣的端點來接收這些事件。 1 export MSSQL\\_JDBC\\_JAR=\u0026#34;/path/to/mssql-jdbc.jar\u0026#34; spark-submit 指令：\n1 2 3 4 5 6 7 8 spark-submit \\ --master local[*] \\ --packages io.openlineage:openlineage-spark:1.13.0 \\ --jars ${MSSQL_JDBC_JAR} \\ --conf \u0026#34;spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\u0026#34; \\ --conf \u0026#34;spark.openlineage.transport.type=http\u0026#34; \\ --conf \u0026#34;spark.openlineage.transport.url=http://your-datahub-gms-host:8080/api/v1/lineage\u0026#34; \\ --conf \u0026#34;spark.openlineage.namespace=my_production_etl\u0026#34; \\ 1 process_mssql_data.py spark.openlineage.transport.url: 設定接收端點，注意！這裡指向的是 Datahub 專門用來接收 OpenLineage 事件的 API 端點。 如以上都執行成功，預計可以抓到資料血緣 (Table to Table)、(Column to Column)， 建議可以 Dev 期間可以先透過 Jupyter Notebook 一行執行， Prod 再進行 Spark Submit。\nSummary 文章中探討了如何透過 PySpark 自動化地將資料血緣傳送到 Datahub。\n核心問題：複雜的資料流程導致可觀測性 (Observability) 低落，難以追蹤和管理。 解決方案：利用 Spark 內建的 SparkListener 機制，在執行期間擷取 Logical Plan，從而自動生成血緣關係。 兩種實現路徑： Datahub 原生整合 (acryl-spark-lineage)：最簡單、最直接的方法，專為 Datahub 設計，適合追求快速實施且技術棧單一的團隊。目前功能上整合越來越多 OpenLineage 的 Feature 做使用。 OpenLineage 開放標準：更具靈活性和擴充性的方法，它將血緣資訊標準化，使你的架構不受特定廠商綁定，是建立企業級、可持續演進資料平台的首選。 如何選擇？\n特性 Datahub 原生整合 (路徑 A) OpenLineage 標準 (路徑 B) 設定難度 ⭐ (極簡) ⭐⭐ (稍有門檻) 廠商中立性 ❌ (綁定 Datahub) ✅ (可發送到任何相容後端) 除錯能力 ⭐⭐ (中等) ⭐⭐⭐⭐⭐ (極佳，可直接查看事件內容) 長期架構 適合單一平台、快速實施 適合企業級、可持續演進的平台 Reference Datahub Native Integration Official Datahub Spark Lineage GitHub: acryl-spark-lineage OpenLineage Standard Official OpenLineage OpenLineage Spark Integration GitHub: openlineage-spark ","date":"2025-09-27T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_lineage_with_datahub/","title":"PySpark 學習筆記 - Data Lineage with Datahub"},{"content":"PySpark - DataFrame Writer 與 Partition Example Overview 參數/方法 作用 對輸出檔案數 對Folder結構 對後續查詢效能 典型用途 repartition(N) 重新Partition（隨機） 控制（≈N 檔） 無影響 無直接幫助 控制檔案數/平衡分工 partitionBy(cols…) 依欄位值分Folder 視Partition值而定 建立 col=value/ 強：Partition Pruning 時間序列、維度篩選 bucketBy(B, cols…)* Hash 分桶（表級） 視Bucket數與計算而定 無（表內邏輯分桶） 中～強：Join/GroupBy 減少 Shuffle 大表 Join/聚合 sortBy(cols…)* Bucket/Partition內排序 無 無 加速Bucket內掃描 時序檢索、範圍查詢 option(\u0026quot;maxRecordsPerFile\u0026quot;, N) 每檔上限筆數 切檔（≤N/檔） 無 無直接幫助 避免小檔/巨檔 * bucketBy/sortBy 只對 saveAsTable 生效，save(path) 無效。 Folder \u0026amp; Bucket 1 2 3 4 5 6 # partitionBy 之後的 Folder /data/out/ds=2025-09-07/part-0000.parquet /data/out/ds=2025-09-08/part-0001.parquet # bucketBy 作用在「表」：沒有 Folder Layer變化，但在 Metastore 中記錄「Bucket」資訊 db.bucketed_events --(16 buckets on user_id, sorted by event_ts) 常見組合與輸出效果 控制檔案數 1 2 3 (df.repartition(32) # 控制輸出 ≈ 32 檔 .write.option(\u0026#34;maxRecordsPerFile\u0026#34;, 2000000) .parquet(\u0026#34;/data/out\u0026#34;)) 檔案數：≈ 32～(更多，若每檔超過 N 筆會再切) 效能：無Partition修剪；單純控顆粒。 時間序列查詢（最佳實踐） 1 2 3 4 5 (df.write .partitionBy(\u0026#34;ds\u0026#34;) # 以日期分Folder .option(\u0026#34;maxRecordsPerFile\u0026#34;, 2_000_000) .mode(\u0026#34;append\u0026#34;) .parquet(\u0026#34;/lake/sales\u0026#34;)) Folder：/lake/sales/ds=YYYY-MM-DD/... 查詢：WHERE ds='2025-09-07' 只掃該日期Partition → 快 大表 Join/GroupBy（OLAP 場景） 1 2 3 4 5 (df.write .mode(\u0026#34;overwrite\u0026#34;) .bucketBy(32, \u0026#34;user_id\u0026#34;) # Table Level Hash 分桶 .sortBy(\u0026#34;event_ts\u0026#34;) .saveAsTable(\u0026#34;warehouse.bucketed_events\u0026#34;)) 效果：與另一張同Bucket數、同 key 的表 Join → 顯著減少 Shuffle 限制：僅 saveAsTable；Bucket數固定，改變需重寫表。 只覆蓋指定Partition（防止整體覆蓋） 1 2 3 4 5 6 spark.conf.set(\u0026#34;spark.sql.sources.partitionOverwriteMode\u0026#34;, \u0026#34;dynamic\u0026#34;) (df.filter(\u0026#34;ds=\u0026#39;2025-09-07\u0026#39;\u0026#34;) .write.partitionBy(\u0026#34;ds\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .parquet(\u0026#34;/lake/sales\u0026#34;)) 結果：只覆蓋 ds=2025-09-07 Partition，不動其他日期。 參數調整建議 需求 推薦做法 備註 減少小檔/控制檔案數 repartition(N) + maxRecordsPerFile N 取決於叢集/資料量 篩選為主（日期/區域） partitionBy(\u0026quot;ds\u0026quot;, \u0026quot;region\u0026quot;) 控制Partition欄位基數，避免爆量Folder 大表頻繁 Join/GroupBy bucketBy(B, key)（+ sortBy） 只用 saveAsTable；雙表Bucket數/鍵一致 時序掃描還要快 partitionBy(\u0026quot;ds\u0026quot;) + 合理檔案大小 搭配下游查詢條件一致 嚴格控制覆蓋範圍 partitionOverwriteMode=dynamic 只覆蓋寫入到的Partition 注意事項 bucketBy/sortBy 對 save(path) 無效；必須 saveAsTable（Hive/Glue/Spark Catalog）。 overwrite 在Partition資料集上若未設定 partitionOverwriteMode=dynamic，可能把整個目標路徑覆蓋掉。 partitionBy 選高基數欄位（如 user_id）會導致Partition爆炸與大量小檔案。 repartition 會 Shuffle；在超大資料集上要留意成本。 maxRecordsPerFile 只控制「每檔筆數」，不控制「檔案大小」；不同格式/壓縮比會有差異。 範例 範例資料集為 Udemy 課程中提供的航班時間資料集\n設定與讀取 Source Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from pyspark.sql import * from pyspark.sql.functions import spark_partition_id from lib.logger import Log4j # 如使用到 avro 記得要去官方下載並確認對應版本 # 筆者使用 Scala 2.13, spark 3.4.3 spark = SparkSession \\ .builder \\ .master(\u0026#34;local[3]\u0026#34;) \\ .appName(\u0026#34;SparkSchemaDemo\u0026#34;) \\ .config(\u0026#34;spark.jars\u0026#34;, \u0026#34;/Users/squid504s/leonard_github/PySpark-Capstone/packages/spark-avro_2.13-3.4.3.jar\u0026#34;) \\ .getOrCreate() logger = Log4j(spark) flightTimeParquetDF = spark.read \\ .format(\u0026#34;parquet\u0026#34;) \\ .load(\u0026#34;dataSource/flight*.parquet\u0026#34;) Default 情況 無 Repartition / 無 PartitionBy 預設情況下 → 單一分區 寫出時只會產生 1 個檔案 查詢時無法進行分區修剪 → 效能較差 1 2 3 4 5 6 7 8 9 logger.info(\u0026#34;Num Partitions before: \u0026#34; + str(flightTimeParquetDF.rdd.getNumPartitions())) flightTimeParquetDF.groupBy(spark_partition_id()).count().show() Result: +--------------------+------+ |SPARK_PARTITION_ID()| count| +--------------------+------+ | 0|470477| +--------------------+------+ 使用 .repartition(5) → 控制輸出檔案數 產生了 5 個 Avro 檔案 但這只是 隨機重新分配資料 → 不會產生實體 Partition Folder 查詢時仍需掃描所有檔案，效能沒優化 1 2 3 4 5 6 7 8 9 10 11 12 13 partitionedDF = flightTimeParquetDF.repartition(5) logger.info(\u0026#34;Num Partitions after: \u0026#34; + str(partitionedDF.rdd.getNumPartitions())) partitionedDF.groupBy(spark_partition_id()).count().show() +--------------------+-----+ |SPARK_PARTITION_ID()|count| +--------------------+-----+ | 0|94096| | 1|94095| | 2|94095| | 3|94095| | 4|94096| +--------------------+-----+ 1 2 3 4 5 partitionedDF.write \\ .format(\u0026#34;avro\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;path\u0026#34;, \u0026#34;/Users/squid504s/leonard_github/PySpark-Capstone/05-DataSinkDemo/dataSinkTest/avro\u0026#34;) \\ .save() 如果想針對 航班運營商(OP_CARRIER) 與 出發地(ORIGIN) 建立實體 Partiotion，可使用 .partitionBy() 讓輸出檔案按欄位值分 Folder Folder 會依照 OP_CARRIER → ORIGIN 建立階層式結構 查詢時可直接針對特定運營商或出發地做 Partition Pruning → 效能大幅提升 1 2 3 4 5 6 7 8 9 flightTimeParquetDF.write \\ .format(\u0026#34;json\u0026#34;) \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .option(\u0026#34;path\u0026#34;, \u0026#34;/Users/squid504s/leonard_github/PySpark-Capstone/05-DataSinkDemo/Avro_test/json/\u0026#34;) \\ .partitionBy(\u0026#34;OP_CARRIER\u0026#34;, \u0026#34;ORIGIN\u0026#34;) \\ .option(\u0026#34;maxRecordsPerFile\u0026#34;, 10000) \\ .save() spark.stop() Reference spark-avro package\nPySpark - Apache Spark Programming in Python for beginners\nApache Spark Official\n","date":"2025-09-20T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_df_writer_api_n_partition/","title":"PySpark 學習筆記 - DataFrame Writer 與 Partition Example"},{"content":"🔥 PySpark - Spark × Log4j2 日誌管理與集中化 (適用於 YARN / Livy / Standalone)\nSpark 與 Log4j 的關係 項目 說明 日誌抽象層 Spark 使用 SLF4J 作為統一日誌 API。 預設日誌系統 Spark 3.2+ 預設採用 Log4j2（舊版為 Log4j 1.x）。 生態一致性 Hadoop、Hive、Kafka、HBase 都用 Log4j，Spark 可無縫整合。 用途 控制日誌等級、輸出格式、檔案位置、輪轉策略、集中收集。 範圍 Spark Driver、Executor、History Server、Livy 均支援統一管理。 為什麼從 Log4j 升級到 Log4j2 特性 Log4j 1.x Log4j2 在 Spark 的好處 效能 同步寫入，效能低 LMAX Disruptor (RingBuffer)，效能快 10 倍 Executor 大量寫 log 不阻塞 AsyncAppender 效率低 原生高效 AsyncAppender 高併發 ETL/Streaming 任務更穩定 JSON 支援 幾乎無 原生 JsonLayout 適合集中式日誌收集 動態調整等級 不支援 支援熱更新 Spark UI 或 REST API 即時切換 log level 安全性 已停止維護 持續更新 避免 Log4Shell 類漏洞 多 Appender 限制較多 支援 Console、File、JSON、Socket 同時輸出多份日誌 適用性 小型應用 分散式叢集友善 Executor 多時效能佳 總結\nSpark 3.2+ 預設使用 Log4j2，更安全、更高效，也避免手動替換相依性問題。 若仍在使用 Spark 2.x（Log4j 1.x），建議升級 Spark 或替換為 Log4j2。 Log4j2 核心概念 元件 功能 範例 Logger 程式中呼叫 log API logger.info(\u0026quot;message\u0026quot;) Appender 決定日誌輸出到哪裡 Console / File / JSON Layout 控制日誌格式 %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1} - %m%n Configuration 控制 Logger、Appender、策略 log4j2.properties 常見 Log Level：\n1 TRACE \u0026lt; DEBUG \u0026lt; INFO \u0026lt; WARN \u0026lt; ERROR \u0026lt; FATAL 建議： Prod → rootLogger = WARN Dev → rootLogger = INFO Log4j2 設定 /etc/spark/log4j2.properties\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 status = WARN name = SparkLog4j2 property.logDir = ${sys:spark.yarn.app.container.log.dir:-/var/log/spark} property.logName = ${sys:logfile.name:-spark-app} property.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %c{1} - %msg%n # Console Appender（Dev 可用） appender.console.type = Console appender.console.name = CONSOLE appender.console.layout.type = PatternLayout appender.console.layout.pattern = ${pattern} # Rolling File Appender appender.rolling.type = RollingFile appender.rolling.name = ROLLING appender.rolling.fileName = ${logDir}/${logName}.log appender.rolling.filePattern = ${logDir}/${logName}.%d{yyyy-MM-dd}.%i.log.gz appender.rolling.layout.type = PatternLayout appender.rolling.layout.pattern = ${pattern} appender.rolling.policies.type = Policies appender.rolling.policies.time.type = TimeBasedTriggeringPolicy appender.rolling.policies.time.interval = 1 appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size = 512MB appender.rolling.strategy.type = DefaultRolloverStrategy appender.rolling.strategy.max = 7 # Async Appender（建議 Prod 開啟） appender.asyncText.type = Async appender.asyncText.name = ASYNC_TEXT appender.asyncText.appenderRef.ref = ROLLING # Root Logger rootLogger.level = WARN rootLogger.appenderRefs = consoleRef, rollingRef rootLogger.appenderRef.consoleRef.ref = CONSOLE rootLogger.appenderRef.rollingRef.ref = ASYNC_TEXT # Parquet、Jetty、Hive logger.parquet.name = org.apache.parquet logger.parquet.level = ERROR logger.jetty.name = org.spark_project.jetty logger.jetty.level = WARN logger.hive.name = org.apache.hadoop.hive.metastore.RetryingHMSHandler logger.hive.level = FATAL # Custom Application logger logger.app.name = com.example.spark logger.app.level = INFO logger.app.additivity = false logger.app.appenderRefs = appConsole, appFile logger.app.appenderRef.appConsole.ref = CONSOLE logger.app.appenderRef.appFile.ref = ASYNC_TEXT YARN 環境集中日誌方案 在 Spark on YARN 模式下，Driver / Executor 分散在不同節點，預設日誌分散於：\n1 2 /var/log/hadoop-yarn/container/\u0026lt;app_id\u0026gt;/stdout /var/log/hadoop-yarn/container/\u0026lt;app_id\u0026gt;/stderr 方案 1：YARN Log Aggregation（最簡單） 適合中小型叢集，無需額外安裝。\n設定步驟 在 yarn-site.xml 啟用：\n1 2 3 4 5 6 7 8 9 10 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.log-aggregation-enable\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 保留 7 天日誌 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.log-aggregation.retain-seconds\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;604800\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 查詢日誌 1 yarn logs -applicationId \u0026lt;app_id\u0026gt; 優點 YARN 原生功能，無需額外元件。 Driver / Executor 日誌會集中到 HDFS。 適合任務結束後查看完整日誌。 缺點 查詢需透過 CLI，無即時監控。 無全文檢索，僅適合單次排錯。 方案 2：Log4j2 Rolling + NFS（共享檔案系統） 適合已有 NAS / NFS ，並希望即時集中日誌。\n設定步驟 設定 Log4j2 輸出目錄 1 2 3 property.logDir = /mnt/shared-logs/spark property.logName = ${sys:logfile.name:-spark-app} appender.rolling.fileName = ${logDir}/${logName}.log spark-submit 指定不同檔名 1 spark-submit --master yarn --conf \u0026#34;spark.driver.extraJavaOptions=-Dlogfile.name=myjob-driver\u0026#34; --conf \u0026#34;spark.executor.extraJavaOptions=-Dlogfile.name=myjob-exec\u0026#34; 優點 所有日誌集中到 /mnt/shared-logs/spark。 可直接 tail -f、grep 即時查 Driver / Executor log。 成本低，部署簡單。 缺點 需要共享檔案系統。 過載時可能影響 Executor 寫入效能。 請確保 /mnt/shared-logs/spark 有正確的讀寫權限，否則 Executor 可能無法寫入日誌。\nSpark on YARN + Log4j2 Logging 總結與建議 Spark Prod 環境請升級到 Log4j2 (≥ 2.17)。 統一路徑 /var/log/spark 或 /mnt/shared-logs/spark → Driver、Executor 日誌統一管理。 低預算最佳方案： YARN Log Aggregation → 集中到 HDFS。 Log4j2 Rolling + NFS → 即時查看日誌。 兩種方案可 同時啟用，兼顧即時監控與日誌歸檔。 Reference Apache Spark 官方 https://spark.apache.org/docs/latest/ Spark Logging https://spark.apache.org/docs/latest/configuration.html#spark-logging Log4j2 \u0026amp; Config https://logging.apache.org/log4j/2.x/manual/configuration.html Spark on YARN https://spark.apache.org/docs/latest/running-on-yarn.html 最近打拳被揍到腦袋有點不靈光 🤕\n寫文章需要咖啡來補血 ☕\n如果你喜歡這篇內容，歡迎請我喝杯咖啡！\nLately I’ve been punched a bit too much in boxing 🥊\nMy brain runs on coffee patches ☕\nIf you enjoyed this post, fuel me with a cup!\n👉 Buy Me a Coffee\n","date":"2025-09-13T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_spark_log4j/","title":"PySpark 學習筆記 - Apache Spark 與 Log4j"},{"content":"SQL Server APPLY 查詢 資料表資訊 (SQL Server) 為了讓範例可以直接執行，我們先建立兩張表：\nCustomers 表 欄位名稱 型別 說明 customer_id INT (PK) 客戶編號 name VARCHAR(20) 客戶名稱 Orders 表 欄位名稱 型別 說明 order_id INT (PK) 訂單編號 customer_id INT (FK) 客戶編號 order_date DATETIME 訂單日期 amount DECIMAL(10,2) 訂單金額 建立資料表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 IF OBJECT_ID(\u0026#39;Orders\u0026#39;, \u0026#39;U\u0026#39;) IS NOT NULL DROP TABLE Orders; IF OBJECT_ID(\u0026#39;Customers\u0026#39;, \u0026#39;U\u0026#39;) IS NOT NULL DROP TABLE Customers; CREATE TABLE Customers ( customer_id INT PRIMARY KEY, name VARCHAR(100) ); CREATE TABLE Orders ( order_id INT PRIMARY KEY, customer_id INT NOT NULL FOREIGN KEY REFERENCES Customers(customer_id), order_date DATETIME NOT NULL, amount DECIMAL(10,2) NOT NULL ); 插入測試資料 1 2 3 4 5 6 7 8 9 10 11 INSERT INTO Customers(customer_id, name) VALUES (1, \u0026#39;Alice\u0026#39;), (2, \u0026#39;Bob\u0026#39;), (3, \u0026#39;Carol\u0026#39;); INSERT INTO Orders(order_id, customer_id, order_date, amount) VALUES (101, 1, \u0026#39;2025-08-01\u0026#39;, 120.00), (102, 1, \u0026#39;2025-08-10\u0026#39;, 90.00), (103, 1, \u0026#39;2025-08-15\u0026#39;, 250.00), (201, 2, \u0026#39;2025-07-20\u0026#39;, 300.00), (202, 2, \u0026#39;2025-08-05\u0026#39;, 80.00); APPLY 原理 在 SQL Server 中，APPLY 是一種 橫向關聯 (lateral join) 的機制。\nCROSS APPLY 像 INNER JOIN OUTER APPLY 像 LEFT JOIN PostgreSQL 對應：LATERAL\nCROSS APPLY vs OUTER APPLY 特性 CROSS APPLY OUTER APPLY 類似於 INNER JOIN LEFT JOIN 如果右邊沒有資料 不回傳該列 保留左邊列，右邊欄位填 NULL 適用場景 只要右邊有結果 即使右邊沒有結果也要顯示 範例 1：每位客戶取最新一筆訂單 CROSS APPLY 1 2 3 4 5 6 7 8 SELECT c.customer_id, c.name, o1.order_id, o1.order_date, o1.amount FROM Customers AS c CROSS APPLY ( SELECT TOP (1) o.* FROM Orders AS o WHERE o.customer_id = c.customer_id ORDER BY o.order_date DESC ) AS o1; OUTER APPLY 1 2 3 4 5 6 7 8 SELECT c.customer_id, c.name, o1.order_id, o1.order_date, o1.amount FROM Customers AS c OUTER APPLY ( SELECT TOP (1) o.* FROM Orders AS o WHERE o.customer_id = c.customer_id ORDER BY o.order_date DESC ) AS o1; 預期結果比較 CROSS APPLY → 只顯示有訂單的客戶\ncustomer_id name order_id order_date amount 1 Alice 103 2025-08-15 250.00 2 Bob 202 2025-08-05 80.00 OUTER APPLY → 保留所有客戶\ncustomer_id name order_id order_date amount 1 Alice 103 2025-08-15 250.00 2 Bob 202 2025-08-05 80.00 3 Carol NULL NULL NULL 範例 2：每位客戶取金額最高前三筆訂單 1 2 3 4 5 6 7 8 9 SELECT c.customer_id, c.name, d.order_id, d.order_date, d.amount FROM Customers AS c OUTER APPLY ( SELECT TOP (3) o.* FROM Orders o WHERE o.customer_id = c.customer_id ORDER BY o.amount DESC ) d ORDER BY c.customer_id, d.amount DESC NULLS LAST; 預期結果\ncustomer_id name order_id order_date amount 1 Alice 103 2025-08-15 250.00 1 Alice 101 2025-08-01 120.00 1 Alice 102 2025-08-10 90.00 2 Bob 201 2025-07-20 300.00 2 Bob 202 2025-08-05 80.00 3 Carol NULL NULL NULL 效能注意事項 APPLY 會針對左表每列執行一次子查詢 建議建立複合索引： 1 2 CREATE INDEX idx_orders_customer_date ON Orders(customer_id, order_date DESC); CROSS APPLY 稍快於 OUTER APPLY Reference 理解 SQL Server 的 CROSS APPLY 和 OUTER APPLY 査詢\n最近打拳被揍到腦袋有點不靈光 🤕\n寫文章需要咖啡來補血 ☕\n如果你喜歡這篇內容，歡迎請我喝杯咖啡！\nLately I’ve been punched a bit too much in boxing 🥊\nMy brain runs on coffee patches ☕\nIf you enjoyed this post, fuel me with a cup!\n👉 Buy Me a Coffee\n","date":"2025-09-06T00:00:00Z","permalink":"https://www.noleonardblog.com/p/sql_server_apply_1/","title":"SQL SERVER `APPLY` 查詢"},{"content":"🔥 PySpark - Basic Exec Model \u0026amp; Resource (以 spark.master=local[3] 為例)\n本筆記涵蓋以下內容：\nSpark 程式執行方式 Spark 運算架構與提交流程 Spark 執行模式與 Cluster Manager Local 模式範例 基本資源調整建議 Spark 程式執行方式 Spark 提供兩大類型的執行方式：互動式開發與提交批次任務。\n互動式開發 (Interactive Clients) 🧪 適合開發與資料探索，快速測試程式與驗證邏輯。\n工具 功能 適用場景 spark-shell Scala / Python / R REPL，快速測試 小型測試、學習 Notebook Jupyter、Zeppelin、Databricks Notebook 資料探索、可視化分析 特點：快速驗證邏輯，但不適合長時間運行或大規模計算。\n提交批次任務 (Submit Job) 🚀 適合正式環境，將 Spark Job 提交給叢集運行。\n工具 功能 適用場景 spark-submit 最常用方式，提交 Application 至叢集 Prod ETL、批次處理 Databricks 雲端 Notebook 平台，內建 Spark 運行環境 雲端數據處理 REST API / Web UI 提交、監控、管理 Spark Job 自動化調度 Spark 運算架構與提交流程 Spark 採用 Driver + Executor 架構，透過 Cluster Manager 管理資源。\n核心元件 元件 類型 功能 Client 提交端 提交 Job，例如 spark-submit Driver JVM Process 任務調度中心，負責 Stage 分割與 Task 分配 Executor JVM Process 執行 Tasks，負責計算資料 Task Thread Executor 內執行的最小計算單位 Cluster Manager 資源管理器 分配叢集 CPU / Memory 資源，啟動 Executors Spark Job 提交流程 Spark 執行模式與 Cluster Manager Spark 支援多種執行模式，決定 Driver 與 Executor 的運行位置。\n模式 spark.master 設定 JVM Process 數量 Thread 數量 適用場景 Local[3] local[3] 1 Driver + 1 Executor 3 本機測試 / 模擬並行 Local[*] local[*] 1 Driver + 1 Executor CPU核心數 壓測或單機極限 Standalone spark://host:7077 多 Executors 多 Threads Spark 原生叢集 YARN yarn Container 決定 多 Threads Hadoop 生態 Kubernetes k8s:// Pod 決定 多 Threads 雲端原生 Mesos mesos:// 多 Executors 多 Threads 大型企業共享叢集 Local 模式範例：spark.master=local[3] local[3] 運行架構圖 1 2 3 - 1 Driver + 1 Executor JVM - Executor 內 3 Threads → 同時處理 3 Tasks - 若 12 Partitions → Spark 需分 4 輪執行 YARN 模式架構圖 Kubernetes 模式架構圖 Spark 可嘗試資源配置策略 模式 Driver 位置 Executor JVM 數 每 Executor Threads 最大併行度 適用場景 local[3] 本機 1 3 3 小型測試 YARN ResourceManager 4 4 16 Hadoop 生態 K8s Pod 4 4 16 雲端原生 最大併行度公式：\n1 Max Concurrent Tasks = Executors × Executor Cores Spark 一些調整建議 Partition 建議大小 ≈ 128MB Executors × Cores ≈ Partition 數 / 2~3 Shuffle Partition = Executors × Cores × 2 避免單 Executor 過多 Threads → 降低 GC 負擔 Production 建議開啟動態資源配置： 1 --conf spark.dynamicAllocation.enabled=true 總結 local[3] → 1 Executor JVM + 3 Threads → 適合開發與模擬並行 Production → 建議使用 YARN / K8s / Standalone Spark 效能調優核心三步： 決定 Partition 數量 設定 Executors × Cores 調整 Shuffle Partitions Reference PySpark - Apache Spark Programming in Python for beginners\n最近打拳被揍到腦袋有點不靈光 🤕\n寫文章需要咖啡來補血 ☕\n如果你喜歡這篇內容，歡迎請我喝杯咖啡！\nLately I’ve been punched a bit too much in boxing 🥊\nMy brain runs on coffee patches ☕\nIf you enjoyed this post, fuel me with a cup!\n👉 Buy Me a Coffee\n","date":"2025-08-29T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_basic_exec_model/","title":"PySpark 學習筆記 - Basic Exec Model \u0026 Resource"},{"content":"🔥 PySpark - Getting Start PySpark 學習\n本筆記涵蓋以下內容：\n建立 Spark Session 讀取 CSV 原始資料 欄位標準化（批次重新命名） 建立暫存表供 SQL 查詢 查詢與轉換範例：週次統計 Spark 官方文件是你個好夥伴 spark.apache.org\nDataFrame Methods\nActions Are DataFrame Operations that kick off a Spark Job execution and return to the Spark Driver collect count describe first foreach foreachPartition head show summary tail take toLocalIterator Transformations Spark DataFrame transformation produces a newly transformed Dataframe agg alias coalesce colRegex crossJoin crosstab cube distinct drop drop_duplicates dropDuplicates dropna exceptAll filter groupby \u0026hellip; Functions/Methods Dataframe Methods or function which are not categorized into Actions or Transformations approxQuantile cache checkpoint createGlobalTempView createOrReplaceGlobalTempView createOrReplaceTempView createTempView explain hint inputFiles isLocal localCheckpoint toDF toJSON \u0026hellip; Dataset - Fire Department\n📦 1. 建立 Spark Session 與讀取資料 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyspark.sql import SparkSession from pyspark.sql.functions import * spark = SparkSession.builder \\ .appName(\u0026#34;basic\u0026#34;) \\ .master(\u0026#34;local[2]\u0026#34;) \\ .getOrCreate() fire_df = spark.read \\ .format(\u0026#34;csv\u0026#34;) \\ .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) \\ .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) \\ .load(\u0026#34;data/sf_file_calls.csv\u0026#34;) 🛠️ 2. 欄位重新命名（標準化） 有些欄位包含空格或不適合程式使用的命名，透過字典映射統一名稱：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 rename_map = { \u0026#34;Call Number\u0026#34;: \u0026#34;CallNumber\u0026#34;, \u0026#34;Unit ID\u0026#34;: \u0026#34;UnitID\u0026#34;, \u0026#34;Incident Number\u0026#34;: \u0026#34;IncidentNumber\u0026#34;, \u0026#34;Call Type\u0026#34;: \u0026#34;CallType\u0026#34;, \u0026#34;Call Date\u0026#34;: \u0026#34;CallDate\u0026#34;, \u0026#34;Watch Date\u0026#34;: \u0026#34;WatchDate\u0026#34;, \u0026#34;Received DtTm\u0026#34;: \u0026#34;ReceivedDtTm\u0026#34;, \u0026#34;Entry DtTm\u0026#34;: \u0026#34;EntryDtTm\u0026#34;, \u0026#34;Dispatch DtTm\u0026#34;: \u0026#34;DispatchDtTm\u0026#34;, \u0026#34;Response DtTm\u0026#34;: \u0026#34;ResponseDtTm\u0026#34;, \u0026#34;On Scene DtTm\u0026#34;: \u0026#34;OnSceneDtTm\u0026#34;, \u0026#34;Transport DtTm\u0026#34;: \u0026#34;TransportDtTm\u0026#34;, \u0026#34;Hospital DtTm\u0026#34;: \u0026#34;HospitalDtTm\u0026#34;, \u0026#34;Call Final Disposition\u0026#34;: \u0026#34;CallFinalDisposition\u0026#34;, \u0026#34;Available DtTm\u0026#34;: \u0026#34;AvailableDtTm\u0026#34;, \u0026#34;Zipcode of Incident\u0026#34;: \u0026#34;ZipcodeofIncident\u0026#34;, \u0026#34;Station Area\u0026#34;: \u0026#34;StationArea\u0026#34;, \u0026#34;Original Priority\u0026#34;: \u0026#34;OriginalPriority\u0026#34;, \u0026#34;FinalPriority\u0026#34;: \u0026#34;FinalPriority\u0026#34;, \u0026#34;ALS Unit\u0026#34;: \u0026#34;ALSUnit\u0026#34;, \u0026#34;Call Type Group\u0026#34;: \u0026#34;CallTypeGroup\u0026#34;, \u0026#34;Number of Alarms\u0026#34;: \u0026#34;NumberofAlarms\u0026#34;, \u0026#34;Unit Type\u0026#34;: \u0026#34;UnitType\u0026#34;, \u0026#34;Unit sequence in call dispatch\u0026#34;: \u0026#34;Unitsequenceincalldispatch\u0026#34;, \u0026#34;Fire Prevention District\u0026#34;: \u0026#34;FirePreventionDistrict\u0026#34;, \u0026#34;Supervisor District\u0026#34;: \u0026#34;SupervisorDistrict\u0026#34;, \u0026#34;Neighborhooods - Analysis Boundaries\u0026#34;: \u0026#34;neighborhoods_analysis_boundaries\u0026#34; } for old, new in rename_map.items(): fire_df = fire_df.withColumnRenamed(old, new) 🧪 3. 建立暫存表供 SQL 查詢 1 fire_df.createOrReplaceTempView(\u0026#34;fire_calls\u0026#34;) 你可以在 Spark SQL 中查詢：\n1 spark.sql(SELECT * FROM fire_calls LIMIT 5).show() 🧾 4. 資料結構與預覽 1 2 fire_df.printSchema() fire_df.show(2, truncate=False, vertical=True) 📊 5. 分析：2018 年每週通報事件數（週次統計） 1 2 3 4 5 6 7 8 9 10 11 12 from pyspark.sql.functions import to_date, year, weekofyear, col result = spark.table(\u0026#34;fire_calls\u0026#34;) \\ .withColumn(\u0026#34;call_date\u0026#34;, to_date(col(\u0026#34;CallDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)) \\ .filter(year(col(\u0026#34;call_date\u0026#34;)) == 2018) \\ .withColumn(\u0026#34;week_of_year\u0026#34;, weekofyear(col(\u0026#34;call_date\u0026#34;))) \\ .groupBy(\u0026#34;week_of_year\u0026#34;) \\ .count() \\ .select(\u0026#34;week_of_year\u0026#34;, \u0026#34;count\u0026#34;) \\ .orderBy(col(\u0026#34;count\u0026#34;).desc()) result.show() 📘 筆記： to_date(...)：轉換字串日期格式 year(...)：擷取年份 weekofyear(...)：擷取週次（1~52） groupBy(...).count()：統計每週通報數 orderBy(...desc())：依數量排序 ✅ 延伸建議 若要輸出為 CSV：\n1 result.write.csv(\u0026#34;output/fire_calls_by_week.csv\u0026#34;, header=True) 若要畫出趨勢圖，可用：\n1 result.toPandas().plot(x=\u0026#34;week_of_year\u0026#34;, y=\u0026#34;count\u0026#34;, kind=\u0026#34;bar\u0026#34;) Reference PySpark - Apache Spark Programming in Python for beginners\n最近打拳被揍到腦袋有點不靈光 🤕\n寫文章需要咖啡來補血 ☕\n如果你喜歡這篇內容，歡迎請我喝杯咖啡！\nLately I’ve been punched a bit too much in boxing 🥊\nMy brain runs on coffee patches ☕\nIf you enjoyed this post, fuel me with a cup!\n👉 Buy Me a Coffee\n","date":"2025-08-23T00:00:00Z","permalink":"https://www.noleonardblog.com/p/pyspark_getting_start/","title":"PySpark 學習筆記 - Getting Start"},{"content":"從我的 Medium 文章轉發: BigQuery Logging \u0026amp; Monitor Google Cloud Logging、Monitoring 和 Alerting 能確保 BigQuery 工作負載可靠運行、優化性能和控制成本的關鍵工具。\n根據 Google Cloud 文件以下可以讓監控優化更多應用面向。\nCloud Monitoring 查看 BigQuery 指標 INFORMATION_SCHEMA views 查看關於 jobs, datasets, tables, reservations 的 metadata Audit Logs 查看各種 Event 像是 (create/delete a table, purchase slots, run jobs)\nCloud Monitoring 具備以下的功能:\n建立儀表板、視覺化圖表 異常的查詢的告警 (ex: 使用者查詢量過大) 分析歷史效能、分析查詢量 Dashboard 基本範例 Dashboard Page Table 大小總覽 Dataset 大小總覽 Cloud Logging Cloud Audit Logs 管理各種事件，像是 Table 的 CRUD。 使用者查詢儲存的數據。 系統生成的系統事件。 Log Viewer LogEntry Objects 可以透過撈取 Logging 資訊作為主動告警 GCP Service Account 使用 BigQuery 查詢量撈取範例 1 2 3 4 5 resource.type=\u0026#34;bigquery_resource\u0026#34; log_name=\u0026#34;projects/\u0026lt;project_id\u0026gt;/logs/cloudaudit.googleapis.com%2Fdata_access\u0026#34; protoPayload.methodName=\u0026#34;jobservice.jobcompleted\u0026#34; protoPayload.serviceData.jobCompletedEvent.job.jobStatistics.totalProcessedBytes \u0026gt; 75000000000 protoPayload.authenticationInfo.principalEmail=~\u0026#34;(iam\\.gserviceaccount\\.com)$\u0026#34; GCP Schedule Query 撈取 Error 範例 1 2 3 4 resource.type=\u0026#34;bigquery_resource\u0026#34; log_name=\u0026#34;projects/\u0026lt;project_id\u0026gt;/logs/cloudaudit.googleapis.com%2Fdata_access\u0026#34; protoPayload.serviceData.jobCompletedEvent.job.jobName.jobId=~\u0026#34;scheduled_query\u0026#34; severity=ERROR 以上撈取出來的可以去做客製化指標並使用 Cloud Alert 告警到 Slack 或是其他通訊軟體。\nLog export to bigquery 可以匯出至 BigQuery 做被動監控報表，並且針對歷史資料做分析。\n有時候 Log Viewer 上較難以閱讀及使用, 可以透過 SQL 的方式查詢。 Audit log 預設保留 30 天，可以將 Log 匯出並針對歷史資料做查詢。 Audit log 匯出時記得設定過濾，避免量過大。 Step:\nBQ 建立資料集 Logging 建立接收器\nBigQuery Logging Table: Reference: Google Cloud OnBoard: 開始建構企業資料倉儲\nINFORMATION SCHEMA Intro\nBigQuery Audit Log\nBigQuery 的三種監控方式 – 匯出 Cloud Logging\n— — — — — — — — — — — — — — — — — — —\n這是一個由 Gemini Advanced 產生的 Summary\nLogging (日誌記錄) 詳細記錄操作歷史： BigQuery 日誌會詳細記錄各種操作，如查詢執行、資料載入、表格建立等。這有助於追蹤作業歷史、調查錯誤原因，並滿足合規審計需求。 深入了解系統行為： 透過分析日誌，您可以深入了解 BigQuery 的運行狀況，識別潛在問題，並優化查詢性能。 長期數據存儲： 日誌可以長期存儲，供未來分析和審計使用。\nMonitoring (監控) 實時系統狀態可視化： 監控提供 BigQuery 關鍵指標的實時可視化，如查詢執行時間、槽位使用率、查詢成本等。這有助於及時發現性能瓶頸和異常情況。 趨勢分析與容量規劃： 透過分析歷史監控數據，您可以了解 BigQuery 使用趨勢，預測未來需求，並相應地規劃容量。 主動問題識別： 監控可以幫助您在問題影響用戶之前主動識別並解決。\nAlerting (警報) 及時通知異常情況： 當 BigQuery 指標超過預設閾值時，警報會及時通知相關人員，以便立即採取行動。 成本控制： 警報可以幫助您監控查詢成本，避免意外的高額費用。 綜上所述，BigQuery Logging、Monitoring 和 Alerting 對於以下方面至關重要：\n優化性能： 識別性能瓶頸，優化查詢和工作負載。 控制成本： 監控和管理查詢成本，避免意外費用。 滿足合規需求： 記錄操作歷史，滿足審計和合規要求。 透過有效利用這些工具，您可以全面管理 BigQuery 環境，確保其高效、可靠地運行，並充分發揮其強大的數據分析能力。\n","date":"2024-11-04T00:00:00Z","image":"https://www.noleonardblog.com/p/bq-log-monitor/c2_hu_4f0b2f6ee2212dca.jpg","permalink":"https://www.noleonardblog.com/p/bq-log-monitor/","title":"BigQuery Logging \u0026 Monitoring"}]