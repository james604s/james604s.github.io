---
title: PySpark å­¸ç¿’ç­†è¨˜ - Getting Start
description: Getting Start å¿«é€Ÿç€è¦½ä¸€é PySpark é–‹ç™¼ç”¨æ³•
slug: pyspark_getting_start
date: 2025-08-29 00:00:00+0000
image: 
categories:
    - PySpark
    - Data Engineering
tags:
    - PySpark
    - Data Engineering
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---

# ğŸ”¥ PySpark - Getting Start 

PySpark å­¸ç¿’

æœ¬ç­†è¨˜æ¶µè“‹ä»¥ä¸‹å…§å®¹ï¼š

- å»ºç«‹ Spark Session
- è®€å– CSV åŸå§‹è³‡æ–™
- æ¬„ä½æ¨™æº–åŒ–ï¼ˆæ‰¹æ¬¡é‡æ–°å‘½åï¼‰
- å»ºç«‹æš«å­˜è¡¨ä¾› SQL æŸ¥è©¢
- æŸ¥è©¢èˆ‡è½‰æ›ç¯„ä¾‹ï¼šé€±æ¬¡çµ±è¨ˆ
---
Spark å®˜æ–¹æ–‡ä»¶æ˜¯ä½ å€‹å¥½å¤¥ä¼´  [spark.apache.org](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/core_classes.html)  
DataFrame Methods
1. Actions
Are DataFrame Operations that kick off a Spark Job execution and return to the Spark Driver
- collect
- count
- describe
- first
- foreach
- foreachPartition
- head
- show
- summary
- tail
- take
- toLocalIterator

2. Transformations
Spark DataFrame transformation produces a newly transformed Dataframe
- agg
- alias
- coalesce
- colRegex
- crossJoin
- crosstab
- cube
- distinct
- drop
- drop_duplicates
- dropDuplicates
- dropna
- exceptAll
- filter
- groupby
...

3. Functions/Methods
Dataframe Methods or function which are not categorized into Actions or Transformations
- approxQuantile
- cache
- checkpoint
- createGlobalTempView
- createOrReplaceGlobalTempView
- createOrReplaceTempView
- createTempView
- explain
- hint
- inputFiles
- isLocal
- localCheckpoint
- toDF
- toJSON
...
---

[Dataset - Fire Department](https://data.sfgov.org/Public-Safety/Fire-Department-and-Emergency-Medical-Services-Dis/nuek-vuh3/about_data)
## ğŸ“¦ 1. å»ºç«‹ Spark Session èˆ‡è®€å–è³‡æ–™

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("basic") \
    .master("local[2]") \
    .getOrCreate()

fire_df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("data/sf_file_calls.csv")
```

---

## ğŸ› ï¸ 2. æ¬„ä½é‡æ–°å‘½åï¼ˆæ¨™æº–åŒ–ï¼‰

æœ‰äº›æ¬„ä½åŒ…å«ç©ºæ ¼æˆ–ä¸é©åˆç¨‹å¼ä½¿ç”¨çš„å‘½åï¼Œé€éå­—å…¸æ˜ å°„çµ±ä¸€åç¨±ï¼š

```python
rename_map = {
    "Call Number": "CallNumber",
    "Unit ID": "UnitID",
    "Incident Number": "IncidentNumber",
    "Call Type": "CallType",
    "Call Date": "CallDate",
    "Watch Date": "WatchDate",
    "Received DtTm": "ReceivedDtTm",
    "Entry DtTm": "EntryDtTm",
    "Dispatch DtTm": "DispatchDtTm",
    "Response DtTm": "ResponseDtTm",
    "On Scene DtTm": "OnSceneDtTm",
    "Transport DtTm": "TransportDtTm",
    "Hospital DtTm": "HospitalDtTm",
    "Call Final Disposition": "CallFinalDisposition",
    "Available DtTm": "AvailableDtTm",
    "Zipcode of Incident": "ZipcodeofIncident",
    "Station Area": "StationArea",
    "Original Priority": "OriginalPriority",
    "FinalPriority": "FinalPriority",
    "ALS Unit": "ALSUnit",
    "Call Type Group": "CallTypeGroup",
    "Number of Alarms": "NumberofAlarms",
    "Unit Type": "UnitType",
    "Unit sequence in call dispatch": "Unitsequenceincalldispatch",
    "Fire Prevention District": "FirePreventionDistrict",
    "Supervisor District": "SupervisorDistrict",
    "Neighborhooods - Analysis Boundaries": "neighborhoods_analysis_boundaries"
}

for old, new in rename_map.items():
    fire_df = fire_df.withColumnRenamed(old, new)
```

---

## ğŸ§ª 3. å»ºç«‹æš«å­˜è¡¨ä¾› SQL æŸ¥è©¢

```python
fire_df.createOrReplaceTempView("fire_calls")
```

ä½ å¯ä»¥åœ¨ Spark SQL ä¸­æŸ¥è©¢ï¼š

```sql
spark.sql(SELECT * FROM fire_calls LIMIT 5).show()
```

---

## ğŸ§¾ 4. è³‡æ–™çµæ§‹èˆ‡é è¦½

```python
fire_df.printSchema()
fire_df.show(2, truncate=False, vertical=True)
```

---

## ğŸ“Š 5. åˆ†æï¼š2018 å¹´æ¯é€±é€šå ±äº‹ä»¶æ•¸ï¼ˆé€±æ¬¡çµ±è¨ˆï¼‰

```python
from pyspark.sql.functions import to_date, year, weekofyear, col

result = spark.table("fire_calls") \
    .withColumn("call_date", to_date(col("CallDate"), "MM/dd/yyyy")) \
    .filter(year(col("call_date")) == 2018) \
    .withColumn("week_of_year", weekofyear(col("call_date"))) \
    .groupBy("week_of_year") \
    .count() \
    .select("week_of_year", "count") \
    .orderBy(col("count").desc())

result.show()
```

### ğŸ“˜ ç­†è¨˜ï¼š

- `to_date(...)`ï¼šè½‰æ›å­—ä¸²æ—¥æœŸæ ¼å¼
- `year(...)`ï¼šæ“·å–å¹´ä»½
- `weekofyear(...)`ï¼šæ“·å–é€±æ¬¡ï¼ˆ1~52ï¼‰
- `groupBy(...).count()`ï¼šçµ±è¨ˆæ¯é€±é€šå ±æ•¸
- `orderBy(...desc())`ï¼šä¾æ•¸é‡æ’åº

---

## âœ… å»¶ä¼¸å»ºè­°

- è‹¥è¦è¼¸å‡ºç‚º CSVï¼š
  ```python
  result.write.csv("output/fire_calls_by_week.csv", header=True)
  ```

- è‹¥è¦ç•«å‡ºè¶¨å‹¢åœ–ï¼Œå¯ç”¨ï¼š
  ```python
  result.toPandas().plot(x="week_of_year", y="count", kind="bar")
  ```

---

## Reference
[PySpark - Apache Spark Programming in Python for beginners](https://www.udemy.com/course/apache-spark-programming-in-python-for-beginners/)
