---
title: PySpark å­¸ç¿’ç­†è¨˜ - DataFrame Writer èˆ‡ Partition Example
description: DataFrame Writer API ä½¿ç”¨èˆ‡åˆ†å€ç¯„ä¾‹æ•ˆæœ
slug: pyspark_df_writer_api_n_partition
date: 2025-09-20 00:00:00+0000
image: 
categories:
    - PySpark
    - Data Engineering
tags:
    - PySpark
    - Data Engineering
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---


# PySpark - DataFrame Writer èˆ‡ Partition Example

## Overview
| åƒæ•¸/æ–¹æ³• | ä½œç”¨ | å°è¼¸å‡ºæª”æ¡ˆæ•¸ | å°Folderçµæ§‹ | å°å¾ŒçºŒæŸ¥è©¢æ•ˆèƒ½ | å…¸å‹ç”¨é€” |
|---|---|---:|---|---|---|
| `repartition(N)` | é‡æ–°Partitionï¼ˆéš¨æ©Ÿï¼‰ | **æ§åˆ¶**ï¼ˆâ‰ˆN æª”ï¼‰ | ç„¡å½±éŸ¿ | ç„¡ç›´æ¥å¹«åŠ© | æ§åˆ¶æª”æ¡ˆæ•¸/å¹³è¡¡åˆ†å·¥ |
| `partitionBy(colsâ€¦)` | ä¾æ¬„ä½å€¼åˆ†Folder | è¦–Partitionå€¼è€Œå®š | **å»ºç«‹ `col=value/`** | **å¼·ï¼šPartition Pruning** | æ™‚é–“åºåˆ—ã€ç¶­åº¦ç¯©é¸ |
| `bucketBy(B, colsâ€¦)`* | Hash åˆ†æ¡¶ï¼ˆè¡¨ç´šï¼‰ | è¦–Bucketæ•¸èˆ‡è¨ˆç®—è€Œå®š | ç„¡ï¼ˆè¡¨å…§é‚è¼¯åˆ†æ¡¶ï¼‰ | **ä¸­ï½å¼·ï¼šJoin/GroupBy æ¸›å°‘ Shuffle** | å¤§è¡¨ Join/èšåˆ |
| `sortBy(colsâ€¦)`* | Bucket/Partitionå…§æ’åº | ç„¡ | ç„¡ | **åŠ é€ŸBucketå…§æƒæ** | æ™‚åºæª¢ç´¢ã€ç¯„åœæŸ¥è©¢ |
| `option("maxRecordsPerFile", N)` | æ¯æª”ä¸Šé™ç­†æ•¸ | **åˆ‡æª”**ï¼ˆâ‰¤N/æª”ï¼‰ | ç„¡ | ç„¡ç›´æ¥å¹«åŠ© | é¿å…å°æª”/å·¨æª” |
\* `bucketBy/sortBy` **åªå° `saveAsTable` ç”Ÿæ•ˆ**ï¼Œ`save(path)` ç„¡æ•ˆã€‚

---

## Folder & Bucket
```
# partitionBy ä¹‹å¾Œçš„ Folder
/data/out/ds=2025-09-07/part-0000.parquet
/data/out/ds=2025-09-08/part-0001.parquet

# bucketBy ä½œç”¨åœ¨ã€Œè¡¨ã€ï¼šæ²’æœ‰ Folder Layerè®ŠåŒ–ï¼Œä½†åœ¨ Metastore ä¸­è¨˜éŒ„ã€ŒBucketã€è³‡è¨Š
db.bucketed_events  --(16 buckets on user_id, sorted by event_ts)
```

---

## å¸¸è¦‹çµ„åˆèˆ‡è¼¸å‡ºæ•ˆæœ

### æ§åˆ¶æª”æ¡ˆæ•¸
```python
(df.repartition(32)                       # æ§åˆ¶è¼¸å‡º â‰ˆ 32 æª”
   .write.option("maxRecordsPerFile", 2000000)
   .parquet("/data/out"))
```
- **æª”æ¡ˆæ•¸**ï¼šâ‰ˆ 32ï½(æ›´å¤šï¼Œè‹¥æ¯æª”è¶…é N ç­†æœƒå†åˆ‡)
- **æ•ˆèƒ½**ï¼šç„¡Partitionä¿®å‰ªï¼›å–®ç´”æ§é¡†ç²’ã€‚

---

### æ™‚é–“åºåˆ—æŸ¥è©¢ï¼ˆæœ€ä½³å¯¦è¸ï¼‰
```python
(df.write
   .partitionBy("ds")                     # ä»¥æ—¥æœŸåˆ†Folder
   .option("maxRecordsPerFile", 2_000_000)
   .mode("append")
   .parquet("/lake/sales"))
```
- **Folder**ï¼š`/lake/sales/ds=YYYY-MM-DD/...`
- **æŸ¥è©¢**ï¼š`WHERE ds='2025-09-07'` åªæƒè©²æ—¥æœŸPartition â†’ **å¿«**

---

### å¤§è¡¨ Join/GroupByï¼ˆOLAP å ´æ™¯ï¼‰
```python
(df.write
   .mode("overwrite")
   .bucketBy(32, "user_id")               # Table Level Hash åˆ†æ¡¶
   .sortBy("event_ts")
   .saveAsTable("warehouse.bucketed_events"))
```
- **æ•ˆæœ**ï¼šèˆ‡å¦ä¸€å¼µåŒBucketæ•¸ã€åŒ key çš„è¡¨ Join â†’ **é¡¯è‘—æ¸›å°‘ Shuffle**
- **é™åˆ¶**ï¼šåƒ… `saveAsTable`ï¼›Bucketæ•¸å›ºå®šï¼Œæ”¹è®Šéœ€é‡å¯«è¡¨ã€‚

---

### åªè¦†è“‹æŒ‡å®šPartitionï¼ˆé˜²æ­¢æ•´é«”è¦†è“‹ï¼‰
```python
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

(df.filter("ds='2025-09-07'")
   .write.partitionBy("ds")
   .mode("overwrite")
   .parquet("/lake/sales"))
```
- **çµæœ**ï¼šåªè¦†è“‹ `ds=2025-09-07` Partitionï¼Œä¸å‹•å…¶ä»–æ—¥æœŸã€‚

---

## åƒæ•¸èª¿æ•´å»ºè­°
| éœ€æ±‚ | æ¨è–¦åšæ³• | å‚™è¨» |
|---|---|---|
| æ¸›å°‘å°æª”/æ§åˆ¶æª”æ¡ˆæ•¸ | `repartition(N)` + `maxRecordsPerFile` | N å–æ±ºæ–¼å¢é›†/è³‡æ–™é‡ |
| ç¯©é¸ç‚ºä¸»ï¼ˆæ—¥æœŸ/å€åŸŸï¼‰ | `partitionBy("ds", "region")` | æ§åˆ¶Partitionæ¬„ä½åŸºæ•¸ï¼Œé¿å…çˆ†é‡Folder |
| å¤§è¡¨é »ç¹ Join/GroupBy | `bucketBy(B, key)`ï¼ˆ+ `sortBy`ï¼‰ | åªç”¨ `saveAsTable`ï¼›é›™è¡¨Bucketæ•¸/éµä¸€è‡´ |
| æ™‚åºæƒæé‚„è¦å¿« | `partitionBy("ds")` + åˆç†æª”æ¡ˆå¤§å° | æ­é…ä¸‹æ¸¸æŸ¥è©¢æ¢ä»¶ä¸€è‡´ |
| åš´æ ¼æ§åˆ¶è¦†è“‹ç¯„åœ | `partitionOverwriteMode=dynamic` | åªè¦†è“‹å¯«å…¥åˆ°çš„Partition |

---

## æ³¨æ„äº‹é …
- `bucketBy/sortBy` **å° `save(path)` ç„¡æ•ˆ**ï¼›å¿…é ˆ `saveAsTable`ï¼ˆHive/Glue/Spark Catalogï¼‰ã€‚  
- `overwrite` åœ¨Partitionè³‡æ–™é›†ä¸Šè‹¥**æœª**è¨­å®š `partitionOverwriteMode=dynamic`ï¼Œå¯èƒ½æŠŠæ•´å€‹ç›®æ¨™è·¯å¾‘è¦†è“‹æ‰ã€‚  
- `partitionBy` é¸é«˜åŸºæ•¸æ¬„ä½ï¼ˆå¦‚ user_idï¼‰æœƒå°è‡´**Partitionçˆ†ç‚¸**èˆ‡å¤§é‡å°æª”æ¡ˆã€‚  
- `repartition` æœƒ Shuffleï¼›åœ¨è¶…å¤§è³‡æ–™é›†ä¸Šè¦ç•™æ„æˆæœ¬ã€‚  
- `maxRecordsPerFile` åªæ§åˆ¶ã€Œæ¯æª”ç­†æ•¸ã€ï¼Œä¸æ§åˆ¶ã€Œæª”æ¡ˆå¤§å°ã€ï¼›ä¸åŒæ ¼å¼/å£“ç¸®æ¯”æœƒæœ‰å·®ç•°ã€‚  

---

## ç¯„ä¾‹
ç¯„ä¾‹è³‡æ–™é›†ç‚º Udemy èª²ç¨‹ä¸­æä¾›çš„èˆªç­æ™‚é–“è³‡æ–™é›†

- è¨­å®šèˆ‡è®€å– Source Data
```python
from pyspark.sql import *
from pyspark.sql.functions import spark_partition_id

from lib.logger import Log4j

# å¦‚ä½¿ç”¨åˆ° avro è¨˜å¾—è¦å»å®˜æ–¹ä¸‹è¼‰ä¸¦ç¢ºèªå°æ‡‰ç‰ˆæœ¬
# ç­†è€…ä½¿ç”¨ Scala 2.13, spark 3.4.3
spark = SparkSession \
    .builder \
    .master("local[3]") \
    .appName("SparkSchemaDemo") \
    .config("spark.jars", "/Users/squid504s/leonard_github/PySpark-Capstone/packages/spark-avro_2.13-3.4.3.jar") \
    .getOrCreate()

logger = Log4j(spark)

flightTimeParquetDF = spark.read \
    .format("parquet") \
    .load("dataSource/flight*.parquet")
```

- Default æƒ…æ³ ç„¡ Repartition / ç„¡ PartitionBy
  - é è¨­æƒ…æ³ä¸‹ â†’ å–®ä¸€åˆ†å€
  - å¯«å‡ºæ™‚åªæœƒç”¢ç”Ÿ 1 å€‹æª”æ¡ˆ
  - æŸ¥è©¢æ™‚ç„¡æ³•é€²è¡Œåˆ†å€ä¿®å‰ª â†’ æ•ˆèƒ½è¼ƒå·®
```python
logger.info("Num Partitions before: " + str(flightTimeParquetDF.rdd.getNumPartitions()))
flightTimeParquetDF.groupBy(spark_partition_id()).count().show()

Result:
+--------------------+------+
|SPARK_PARTITION_ID()| count|
+--------------------+------+
|                   0|470477|
+--------------------+------+
```

- ä½¿ç”¨ .repartition(5) â†’ æ§åˆ¶è¼¸å‡ºæª”æ¡ˆæ•¸
  - ç”¢ç”Ÿäº† 5 å€‹ Avro æª”æ¡ˆ
  - ä½†é€™åªæ˜¯ éš¨æ©Ÿé‡æ–°åˆ†é…è³‡æ–™ â†’ ä¸æœƒç”¢ç”Ÿå¯¦é«” Partition Folder
  - æŸ¥è©¢æ™‚ä»éœ€æƒææ‰€æœ‰æª”æ¡ˆï¼Œæ•ˆèƒ½æ²’å„ªåŒ–
```python
partitionedDF = flightTimeParquetDF.repartition(5)
logger.info("Num Partitions after: " + str(partitionedDF.rdd.getNumPartitions()))
partitionedDF.groupBy(spark_partition_id()).count().show()

+--------------------+-----+
|SPARK_PARTITION_ID()|count|
+--------------------+-----+
|                   0|94096|
|                   1|94095|
|                   2|94095|
|                   3|94095|
|                   4|94096|
+--------------------+-----+
```
```
partitionedDF.write \
    .format("avro") \
    .mode("overwrite") \
    .option("path", "/Users/squid504s/leonard_github/PySpark-Capstone/05-DataSinkDemo/dataSinkTest/avro") \
    .save()
```
![partition](partition1.png)

- å¦‚æœæƒ³é‡å° èˆªç­é‹ç‡Ÿå•†(OP_CARRIER) èˆ‡ å‡ºç™¼åœ°(ORIGIN) å»ºç«‹å¯¦é«” Partiotionï¼Œå¯ä½¿ç”¨ .partitionBy() è®“è¼¸å‡ºæª”æ¡ˆæŒ‰æ¬„ä½å€¼åˆ† Folder
  - Folder æœƒä¾ç…§ OP_CARRIER â†’ ORIGIN å»ºç«‹éšå±¤å¼çµæ§‹
  - æŸ¥è©¢æ™‚å¯ç›´æ¥é‡å°ç‰¹å®šé‹ç‡Ÿå•†æˆ–å‡ºç™¼åœ°åš Partition Pruning â†’ æ•ˆèƒ½å¤§å¹…æå‡
```
flightTimeParquetDF.write \
    .format("json") \
    .mode("overwrite") \
    .option("path", "/Users/squid504s/leonard_github/PySpark-Capstone/05-DataSinkDemo/Avro_test/json/") \
    .partitionBy("OP_CARRIER", "ORIGIN") \
    .option("maxRecordsPerFile", 10000) \
    .save()

spark.stop()
```
![partition2](partition2.png)

## Reference
[spark-avro package](https://central.sonatype.com/artifact/org.apache.spark/spark-avro_2.13/3.4.3/versions)

[PySpark - Apache Spark Programming in Python for beginners](https://www.udemy.com/course/apache-spark-programming-in-python-for-beginners/)

[Apache Spark Official](https://spark.apache.org/docs/latest/)



---

æ‹³é¤¨è¦æ¬å®¶äº†ï¼Œæ›åˆ°æ–°åœ°é»ï¼Œæœ‰æ“‚å°å¥½èˆˆå¥®è€¶ï¼

The gym is moving to a new location, and I'm so excited because there's going to be a boxing ring! ğŸ¥Š

ğŸ‘‰ [Buy Me a Coffee](https://buymeacoffee.com/james604s)

---