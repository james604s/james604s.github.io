---
title: PySpark å­¸ç¿’ç­†è¨˜ - Apache Spark èˆ‡ Log4j
description: å¿«é€Ÿäº†è§£ Spark èˆ‡ Log4j çš„é—œä¿‚èˆ‡åŸºæœ¬è¨­å®š
slug: pyspark_spark_log4j
date: 2025-09-13 00:00:00+0000
image: 
categories:
    - PySpark
    - Data Engineering
tags:
    - PySpark
    - Data Engineering
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---
# ğŸ”¥ PySpark - Spark Ã— Log4j2 æ—¥èªŒç®¡ç†èˆ‡é›†ä¸­åŒ–

*(é©ç”¨æ–¼ YARN / Livy / Standalone)*

## **Spark èˆ‡ Log4j çš„é—œä¿‚**

| é …ç›® | èªªæ˜ |
|------|------|
| **æ—¥èªŒæŠ½è±¡å±¤** | Spark ä½¿ç”¨ **SLF4J** ä½œç‚ºçµ±ä¸€æ—¥èªŒ APIã€‚ |
| **é è¨­æ—¥èªŒç³»çµ±** | Spark 3.2+ é è¨­æ¡ç”¨ **Log4j2**ï¼ˆèˆŠç‰ˆç‚º Log4j 1.xï¼‰ã€‚ |
| **ç”Ÿæ…‹ä¸€è‡´æ€§** | Hadoopã€Hiveã€Kafkaã€HBase éƒ½ç”¨ Log4jï¼ŒSpark å¯ç„¡ç¸«æ•´åˆã€‚ |
| **ç”¨é€”** | æ§åˆ¶æ—¥èªŒç­‰ç´šã€è¼¸å‡ºæ ¼å¼ã€æª”æ¡ˆä½ç½®ã€è¼ªè½‰ç­–ç•¥ã€é›†ä¸­æ”¶é›†ã€‚ |
| **ç¯„åœ** | Spark Driverã€Executorã€History Serverã€Livy å‡æ”¯æ´çµ±ä¸€ç®¡ç†ã€‚ |

---

## **ç‚ºä»€éº¼å¾ Log4j å‡ç´šåˆ° Log4j2**

| ç‰¹æ€§ | Log4j 1.x | Log4j2 | åœ¨ Spark çš„å¥½è™• |
|------|-----------|---------|------------------|
| **æ•ˆèƒ½** | åŒæ­¥å¯«å…¥ï¼Œæ•ˆèƒ½ä½ | **LMAX Disruptor (RingBuffer)**ï¼Œæ•ˆèƒ½å¿« **10 å€** | Executor å¤§é‡å¯« log ä¸é˜»å¡ |
| **AsyncAppender** | æ•ˆç‡ä½ | **åŸç”Ÿé«˜æ•ˆ AsyncAppender** | é«˜ä½µç™¼ ETL/Streaming ä»»å‹™æ›´ç©©å®š |
| **JSON æ”¯æ´** | å¹¾ä¹ç„¡ | **åŸç”Ÿ JsonLayout** | é©åˆé›†ä¸­å¼æ—¥èªŒæ”¶é›† |
| **å‹•æ…‹èª¿æ•´ç­‰ç´š** | ä¸æ”¯æ´ | **æ”¯æ´ç†±æ›´æ–°** | Spark UI æˆ– REST API å³æ™‚åˆ‡æ› log level |
| **å®‰å…¨æ€§** | å·²åœæ­¢ç¶­è­· | **æŒçºŒæ›´æ–°** | é¿å… **Log4Shell** é¡æ¼æ´ |
| **å¤š Appender** | é™åˆ¶è¼ƒå¤š | æ”¯æ´ Consoleã€Fileã€JSONã€Socket | åŒæ™‚è¼¸å‡ºå¤šä»½æ—¥èªŒ |
| **é©ç”¨æ€§** | å°å‹æ‡‰ç”¨ | **åˆ†æ•£å¼å¢é›†å‹å–„** | Executor å¤šæ™‚æ•ˆèƒ½ä½³ |

> **ç¸½çµ**  
- Spark 3.2+ é è¨­ä½¿ç”¨ **Log4j2**ï¼Œæ›´å®‰å…¨ã€æ›´é«˜æ•ˆï¼Œä¹Ÿé¿å…æ‰‹å‹•æ›¿æ›ç›¸ä¾æ€§å•é¡Œã€‚  
- è‹¥ä»åœ¨ä½¿ç”¨ Spark 2.xï¼ˆLog4j 1.xï¼‰ï¼Œå»ºè­°å‡ç´š Spark æˆ–æ›¿æ›ç‚º Log4j2ã€‚

---

## **Log4j2 æ ¸å¿ƒæ¦‚å¿µ**

| å…ƒä»¶ | åŠŸèƒ½ | ç¯„ä¾‹ |
|------|------|----------------|
| **Logger** | ç¨‹å¼ä¸­å‘¼å« log API | `logger.info("message")` |
| **Appender** | æ±ºå®šæ—¥èªŒè¼¸å‡ºåˆ°å“ªè£¡ | Console / File / JSON |
| **Layout** | æ§åˆ¶æ—¥èªŒæ ¼å¼ | `%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1} - %m%n` |
| **Configuration** | æ§åˆ¶ Loggerã€Appenderã€ç­–ç•¥ | `log4j2.properties` |

**å¸¸è¦‹ Log Levelï¼š**
```
TRACE < DEBUG < INFO < WARN < ERROR < FATAL
```
- å»ºè­°ï¼š
    - **Prod** â†’ `rootLogger = WARN`
    - **Dev** â†’ `rootLogger = INFO`

---

## **Log4j2 è¨­å®š**

`/etc/spark/log4j2.properties`
```properties
status = WARN
name = SparkLog4j2
property.logDir = ${sys:spark.yarn.app.container.log.dir:-/var/log/spark}
property.logName = ${sys:logfile.name:-spark-app}
property.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %c{1} - %msg%n

# Console Appenderï¼ˆDev å¯ç”¨ï¼‰
appender.console.type = Console
appender.console.name = CONSOLE
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = ${pattern}

# Rolling File Appender
appender.rolling.type = RollingFile
appender.rolling.name = ROLLING
appender.rolling.fileName = ${logDir}/${logName}.log
appender.rolling.filePattern = ${logDir}/${logName}.%d{yyyy-MM-dd}.%i.log.gz
appender.rolling.layout.type = PatternLayout
appender.rolling.layout.pattern = ${pattern}
appender.rolling.policies.type = Policies
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
appender.rolling.policies.time.interval = 1
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
appender.rolling.policies.size.size = 512MB
appender.rolling.strategy.type = DefaultRolloverStrategy
appender.rolling.strategy.max = 7

# Async Appenderï¼ˆå»ºè­° Prod é–‹å•Ÿï¼‰
appender.asyncText.type = Async
appender.asyncText.name = ASYNC_TEXT
appender.asyncText.appenderRef.ref = ROLLING

# Root Logger
rootLogger.level = WARN
rootLogger.appenderRefs = consoleRef, rollingRef
rootLogger.appenderRef.consoleRef.ref = CONSOLE
rootLogger.appenderRef.rollingRef.ref = ASYNC_TEXT

# Parquetã€Jettyã€Hive
logger.parquet.name = org.apache.parquet
logger.parquet.level = ERROR
logger.jetty.name = org.spark_project.jetty
logger.jetty.level = WARN
logger.hive.name = org.apache.hadoop.hive.metastore.RetryingHMSHandler
logger.hive.level = FATAL

# Custom Application logger
logger.app.name = com.example.spark
logger.app.level = INFO
logger.app.additivity = false
logger.app.appenderRefs = appConsole, appFile
logger.app.appenderRef.appConsole.ref = CONSOLE
logger.app.appenderRef.appFile.ref = ASYNC_TEXT
```

---

## **YARN ç’°å¢ƒé›†ä¸­æ—¥èªŒæ–¹æ¡ˆ**

åœ¨ **Spark on YARN** æ¨¡å¼ä¸‹ï¼ŒDriver / Executor åˆ†æ•£åœ¨ä¸åŒç¯€é»ï¼Œé è¨­æ—¥èªŒåˆ†æ•£æ–¼ï¼š
```
/var/log/hadoop-yarn/container/<app_id>/stdout
/var/log/hadoop-yarn/container/<app_id>/stderr
```

---

### **æ–¹æ¡ˆ 1ï¼šYARN Log Aggregationï¼ˆæœ€ç°¡å–®ï¼‰**
> é©åˆä¸­å°å‹å¢é›†ï¼Œç„¡éœ€é¡å¤–å®‰è£ã€‚

#### **è¨­å®šæ­¥é©Ÿ**
åœ¨ `yarn-site.xml` å•Ÿç”¨ï¼š
```xml
<property>
  <name>yarn.log-aggregation-enable</name>
  <value>true</value>
</property>

<!-- ä¿ç•™ 7 å¤©æ—¥èªŒ -->
<property>
  <name>yarn.log-aggregation.retain-seconds</name>
  <value>604800</value>
</property>
```

#### **æŸ¥è©¢æ—¥èªŒ**
```bash
yarn logs -applicationId <app_id>
```

#### **å„ªé»**
- YARN åŸç”ŸåŠŸèƒ½ï¼Œç„¡éœ€é¡å¤–å…ƒä»¶ã€‚
- Driver / Executor æ—¥èªŒæœƒé›†ä¸­åˆ° **HDFS**ã€‚
- é©åˆä»»å‹™çµæŸå¾ŒæŸ¥çœ‹å®Œæ•´æ—¥èªŒã€‚

#### **ç¼ºé»**
- æŸ¥è©¢éœ€é€é CLIï¼Œç„¡å³æ™‚ç›£æ§ã€‚
- ç„¡å…¨æ–‡æª¢ç´¢ï¼Œåƒ…é©åˆå–®æ¬¡æ’éŒ¯ã€‚

---

### **æ–¹æ¡ˆ 2ï¼šLog4j2 Rolling + NFSï¼ˆå…±äº«æª”æ¡ˆç³»çµ±ï¼‰**
> é©åˆå·²æœ‰ NAS / NFS ï¼Œä¸¦å¸Œæœ›å³æ™‚é›†ä¸­æ—¥èªŒã€‚

#### **è¨­å®šæ­¥é©Ÿ**
1. **è¨­å®š Log4j2 è¼¸å‡ºç›®éŒ„**
```properties
property.logDir = /mnt/shared-logs/spark
property.logName = ${sys:logfile.name:-spark-app}
appender.rolling.fileName = ${logDir}/${logName}.log
```
2. **spark-submit æŒ‡å®šä¸åŒæª”å**
```bash
spark-submit   --master yarn   --conf "spark.driver.extraJavaOptions=-Dlogfile.name=myjob-driver"   --conf "spark.executor.extraJavaOptions=-Dlogfile.name=myjob-exec"
```

#### **å„ªé»**
- æ‰€æœ‰æ—¥èªŒé›†ä¸­åˆ° `/mnt/shared-logs/spark`ã€‚
- å¯ç›´æ¥ `tail -f`ã€`grep` å³æ™‚æŸ¥ Driver / Executor logã€‚
- æˆæœ¬ä½ï¼Œéƒ¨ç½²ç°¡å–®ã€‚

#### **ç¼ºé»**
- éœ€è¦å…±äº«æª”æ¡ˆç³»çµ±ã€‚
- éè¼‰æ™‚å¯èƒ½å½±éŸ¿ Executor å¯«å…¥æ•ˆèƒ½ã€‚

>è«‹ç¢ºä¿ /mnt/shared-logs/spark æœ‰æ­£ç¢ºçš„è®€å¯«æ¬Šé™ï¼Œå¦å‰‡ Executor å¯èƒ½ç„¡æ³•å¯«å…¥æ—¥èªŒã€‚
---

## **Spark on YARN + Log4j2 Logging**

![Structure](log4j.png)

---

## **ç¸½çµèˆ‡å»ºè­°**

- Spark Prod ç’°å¢ƒè«‹å‡ç´šåˆ° **Log4j2 (â‰¥ 2.17)**ã€‚
- çµ±ä¸€è·¯å¾‘ `/var/log/spark` æˆ– `/mnt/shared-logs/spark` â†’ Driverã€Executor æ—¥èªŒçµ±ä¸€ç®¡ç†ã€‚
- **ä½é ç®—æœ€ä½³æ–¹æ¡ˆ**ï¼š
    1. **YARN Log Aggregation** â†’ é›†ä¸­åˆ° HDFSã€‚
    2. **Log4j2 Rolling + NFS** â†’ å³æ™‚æŸ¥çœ‹æ—¥èªŒã€‚
- å…©ç¨®æ–¹æ¡ˆå¯ **åŒæ™‚å•Ÿç”¨**ï¼Œå…¼é¡§å³æ™‚ç›£æ§èˆ‡æ—¥èªŒæ­¸æª”ã€‚

---

## **Reference**

|  |  |
|------|------|
| Apache Spark å®˜æ–¹ | https://spark.apache.org/docs/latest/ |
| Spark Logging | https://spark.apache.org/docs/latest/configuration.html#spark-logging |
| Log4j2 & Config | https://logging.apache.org/log4j/2.x/manual/configuration.html |
| Spark on YARN | https://spark.apache.org/docs/latest/running-on-yarn.html |
