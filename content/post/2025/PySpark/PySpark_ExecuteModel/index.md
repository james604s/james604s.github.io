---
title: PySpark å­¸ç¿’ç­†è¨˜ - Basic Exec Model & Resource
description: åŸºæœ¬åŸ·è¡Œæ¶æ§‹åŠè³‡æºèª¿æ•´
slug: pyspark_basic_exec_model
date: 2025-08-29 00:00:00+0000
image: 
categories:
    - PySpark
    - Data Engineering
tags:
    - PySpark
    - Data Engineering
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---

# ğŸ”¥ PySpark - Basic Exec Model & Resource
*(ä»¥ `spark.master=local[3]` ç‚ºä¾‹)*

---

æœ¬ç­†è¨˜æ¶µè“‹ä»¥ä¸‹å…§å®¹ï¼š

- Spark ç¨‹å¼åŸ·è¡Œæ–¹å¼
- Spark é‹ç®—æ¶æ§‹èˆ‡æäº¤æµç¨‹
- Spark åŸ·è¡Œæ¨¡å¼èˆ‡ Cluster Manager
- Local æ¨¡å¼ç¯„ä¾‹
- åŸºæœ¬è³‡æºèª¿æ•´å»ºè­°

## 1. Spark ç¨‹å¼åŸ·è¡Œæ–¹å¼

Spark æä¾›å…©å¤§é¡å‹çš„åŸ·è¡Œæ–¹å¼ï¼š**äº’å‹•å¼é–‹ç™¼**èˆ‡**æäº¤æ‰¹æ¬¡ä»»å‹™**ã€‚

### 1.1 äº’å‹•å¼é–‹ç™¼ (Interactive Clients) ğŸ§ª
é©åˆé–‹ç™¼èˆ‡è³‡æ–™æ¢ç´¢ï¼Œå¿«é€Ÿæ¸¬è©¦ç¨‹å¼èˆ‡é©—è­‰é‚è¼¯ã€‚

| å·¥å…· | åŠŸèƒ½ | é©ç”¨å ´æ™¯ |
|------|------|-----------|
| **spark-shell** | Scala / Python / R REPLï¼Œå¿«é€Ÿæ¸¬è©¦ | å°å‹æ¸¬è©¦ã€å­¸ç¿’ |
| **Notebook** | Jupyterã€Zeppelinã€Databricks Notebook | è³‡æ–™æ¢ç´¢ã€å¯è¦–åŒ–åˆ†æ |

> **ç‰¹é»**ï¼šå¿«é€Ÿé©—è­‰é‚è¼¯ï¼Œä½†**ä¸é©åˆé•·æ™‚é–“é‹è¡Œ**æˆ–å¤§è¦æ¨¡è¨ˆç®—ã€‚

---

### 1.2 æäº¤æ‰¹æ¬¡ä»»å‹™ (Submit Job) ğŸš€
é©åˆæ­£å¼ç’°å¢ƒï¼Œå°‡ Spark Job æäº¤çµ¦å¢é›†é‹è¡Œã€‚

| å·¥å…· | åŠŸèƒ½ | é©ç”¨å ´æ™¯ |
|------|------|-----------|
| **spark-submit** | æœ€å¸¸ç”¨æ–¹å¼ï¼Œæäº¤ Application è‡³å¢é›† | ç”Ÿç”¢ ETLã€æ‰¹æ¬¡è™•ç† |
| **Databricks** | é›²ç«¯ Notebook å¹³å°ï¼Œå…§å»º Spark é‹è¡Œç’°å¢ƒ | é›²ç«¯æ•¸æ“šè™•ç† |
| **REST API / Web UI** | æäº¤ã€ç›£æ§ã€ç®¡ç† Spark Job | è‡ªå‹•åŒ–èª¿åº¦ |

---

## 2. Spark é‹ç®—æ¶æ§‹èˆ‡æäº¤æµç¨‹

Spark æ¡ç”¨ **Driver + Executor** æ¶æ§‹ï¼Œé€é **Cluster Manager** ç®¡ç†è³‡æºã€‚

### 2.1 æ ¸å¿ƒå…ƒä»¶
| å…ƒä»¶ | é¡å‹ | åŠŸèƒ½ |
|------|------|------|
| **Client** | æäº¤ç«¯ | æäº¤ Jobï¼Œä¾‹å¦‚ `spark-submit` |
| **Driver** | JVM Process | ä»»å‹™èª¿åº¦ä¸­å¿ƒï¼Œè² è²¬ Stage åˆ†å‰²èˆ‡ Task åˆ†é… |
| **Executor** | JVM Process | åŸ·è¡Œ Tasksï¼Œè² è²¬è¨ˆç®—è³‡æ–™ |
| **Task** | Thread | Executor å…§åŸ·è¡Œçš„æœ€å°è¨ˆç®—å–®ä½ |
| **Cluster Manager** | è³‡æºç®¡ç†å™¨ | åˆ†é…å¢é›† CPU / Memory è³‡æºï¼Œå•Ÿå‹• Executors |

---

### 2.2 Spark Job æäº¤æµç¨‹

```mermaid
flowchart TB
    subgraph Client
        C[spark-submit / Notebook]
    end
    C --> D[Driver JVM]
    D --> CM[Cluster Manager - Standalone, YARN, K8s]
    
    CM --> E1[Executor 1 JVM]
    CM --> E2[Executor 2 JVM]
    CM --> E3[Executor 3 JVM]
    
    E1 --> T1[Task 1] --> P1[Partition 1]
    E1 --> T2[Task 2] --> P2[Partition 2]
    E2 --> T3[Task 3] --> P3[Partition 3]
```

---

## 3. Spark åŸ·è¡Œæ¨¡å¼èˆ‡ Cluster Manager

Spark æ”¯æ´å¤šç¨®åŸ·è¡Œæ¨¡å¼ï¼Œæ±ºå®š Driver èˆ‡ Executor çš„é‹è¡Œä½ç½®ã€‚

| æ¨¡å¼ | spark.master è¨­å®š | JVM Process æ•¸é‡ | Thread æ•¸é‡ | é©ç”¨å ´æ™¯ |
|------|--------------------|------------------|-------------|-----------|
| **Local[3]** | `local[3]` | 1 Driver + 1 Executor | 3 | æœ¬æ©Ÿæ¸¬è©¦ / æ¨¡æ“¬ä¸¦è¡Œ |
| **Local[*]** | `local[*]` | 1 Driver + 1 Executor | CPUæ ¸å¿ƒæ•¸ | å£“æ¸¬æˆ–å–®æ©Ÿæ¥µé™ |
| **Standalone** | `spark://host:7077` | å¤š Executors | å¤š Threads | Spark åŸç”Ÿå¢é›† |
| **YARN** | `yarn` | Container æ±ºå®š | å¤š Threads | Hadoop ç”Ÿæ…‹ |
| **Kubernetes** | `k8s://` | Pod æ±ºå®š | å¤š Threads | é›²ç«¯åŸç”Ÿ |
| **Mesos** | `mesos://` | å¤š Executors | å¤š Threads | å¤§å‹ä¼æ¥­å…±äº«å¢é›† |

---

## 4. Local æ¨¡å¼ç¯„ä¾‹ï¼šspark.master=local[3]

### 4.1 local[3] é‹è¡Œæ¶æ§‹åœ–
```mermaid
graph TD
    A[Driver JVM] --> B[Executor JVM]
    B --> T1[Task 1, Thread 1]
    B --> T2[Task 2, Thread 2]
    B --> T3[Task 3, Thread 3]
    T1 --> P1[Partition 1]
    T2 --> P2[Partition 2]
    T3 --> P3[Partition 3]
```

> **é‡é»**  
- 1 Driver + 1 Executor JVM  
- Executor å…§ 3 Threads â†’ åŒæ™‚è™•ç† 3 Tasks  
- è‹¥ 12 Partitions â†’ Spark éœ€åˆ† 4 è¼ªåŸ·è¡Œ

---

## 5. YARN æ¨¡å¼æ¶æ§‹åœ–

```mermaid
flowchart TB
    subgraph Driver
      D[Driver JVM]
    end

    subgraph YARN[Cluster Manager - YARN]
      D --> RM[ResourceManager]
    end

    RM --> E1[Executor 1 JVM]
    RM --> E2[Executor 2 JVM]
    RM --> E3[Executor 3 JVM]
    RM --> E4[Executor 4 JVM]

    E1 --> T1_1[Task 1] --> P1[Partition 1]
    E1 --> T1_2[Task 2] --> P2[Partition 2]
    E1 --> T1_3[Task 3] --> P3[Partition 3]
    E1 --> T1_4[Task 4] --> P4[Partition 4]

    E2 --> T2_1[Task 5] --> P5[Partition 5]
    E2 --> T2_2[Task 6] --> P6[Partition 6]
    E2 --> T2_3[Task 7] --> P7[Partition 7]
    E2 --> T2_4[Task 8] --> P8[Partition 8]
```

---

## 6. Kubernetes æ¨¡å¼æ¶æ§‹åœ–

```mermaid
flowchart TB
    subgraph DriverPod
        D[Driver Pod]
    end

    D --> API[Kubernetes API Server]

    API --> P1[Executor Pod 1, 4 cores]
    API --> P2[Executor Pod 2, 4 cores]
    API --> P3[Executor Pod 3, 4 cores]
    API --> P4[Executor Pod 4, 4 cores]

    P1 --> K1[Task 1]
    P1 --> K2[Task 2]
    P1 --> K3[Task 3]
    P1 --> K4[Task 4]

    P2 --> K5[Task 5]
    P2 --> K6[Task 6]
    P2 --> K7[Task 7]
    P2 --> K8[Task 8]
```

---

## 7. Spark å¯å˜—è©¦è³‡æºé…ç½®ç­–ç•¥

| æ¨¡å¼ | Driver ä½ç½® | Executor JVM æ•¸ | æ¯ Executor Threads | æœ€å¤§ä½µè¡Œåº¦ | é©ç”¨å ´æ™¯ |
|------|------------|-----------------|---------------------|-----------|-----------|
| **local[3]** | æœ¬æ©Ÿ | 1 | 3 | 3 | å°å‹æ¸¬è©¦ |
| **YARN** | ResourceManager | 4 | 4 | 16 | Hadoop ç”Ÿæ…‹ |
| **K8s** | Pod | 4 | 4 | 16 | é›²ç«¯åŸç”Ÿ |

**æœ€å¤§ä½µè¡Œåº¦å…¬å¼ï¼š**
```text
Max Concurrent Tasks = Executors Ã— Executor Cores
```

---

## 8. Spark ä¸€äº›èª¿æ•´å»ºè­°

1. **Partition** å»ºè­°å¤§å° â‰ˆ 128MB  
2. **Executors Ã— Cores** â‰ˆ Partition æ•¸ / 2~3  
3. **Shuffle Partition** = Executors Ã— Cores Ã— 2  
4. é¿å…å–® Executor éå¤š Threads â†’ é™ä½ GC è² æ“”  
5. ç”Ÿç”¢å»ºè­°é–‹å•Ÿå‹•æ…‹è³‡æºé…ç½®ï¼š  
```bash
--conf spark.dynamicAllocation.enabled=true
```

---

## 9. ç¸½çµ

- **local[3]** â†’ 1 Executor JVM + 3 Threads â†’ é©åˆé–‹ç™¼èˆ‡æ¨¡æ“¬ä¸¦è¡Œ  
- ç”Ÿç”¢ç’°å¢ƒ â†’ å»ºè­°ä½¿ç”¨ **YARN / K8s / Standalone**  
- Spark æ•ˆèƒ½èª¿å„ªæ ¸å¿ƒä¸‰æ­¥ï¼š
    1. æ±ºå®š Partition æ•¸é‡
    2. è¨­å®š Executors Ã— Cores
    3. èª¿æ•´ Shuffle Partitions

---

## Reference
[PySpark - Apache Spark Programming in Python for beginners](https://www.udemy.com/course/apache-spark-programming-in-python-for-beginners/)