---
title: PySpark å­¸ç¿’ç­†è¨˜ - Basic Exec Model & Resource
description: åŸºæœ¬åŸ·è¡Œæ¶æ§‹åŠè³‡æºèª¿æ•´
slug: pyspark_basic_exec_model
date: 2025-08-29 00:00:00+0000
image: 
categories:
    - PySpark
    - Data Engineering
tags:
    - PySpark
    - Data Engineering
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---

# ğŸ”¥ PySpark - Basic Exec Model & Resource
*(ä»¥ `spark.master=local[3]` ç‚ºä¾‹)*

---

æœ¬ç­†è¨˜æ¶µè“‹ä»¥ä¸‹å…§å®¹ï¼š

- Spark ç¨‹å¼åŸ·è¡Œæ–¹å¼
- Spark é‹ç®—æ¶æ§‹èˆ‡æäº¤æµç¨‹
- Spark åŸ·è¡Œæ¨¡å¼èˆ‡ Cluster Manager
- Local æ¨¡å¼ç¯„ä¾‹
- åŸºæœ¬è³‡æºèª¿æ•´å»ºè­°

## Spark ç¨‹å¼åŸ·è¡Œæ–¹å¼

Spark æä¾›å…©å¤§é¡å‹çš„åŸ·è¡Œæ–¹å¼ï¼š**äº’å‹•å¼é–‹ç™¼**èˆ‡**æäº¤æ‰¹æ¬¡ä»»å‹™**ã€‚

### äº’å‹•å¼é–‹ç™¼ (Interactive Clients) ğŸ§ª
é©åˆé–‹ç™¼èˆ‡è³‡æ–™æ¢ç´¢ï¼Œå¿«é€Ÿæ¸¬è©¦ç¨‹å¼èˆ‡é©—è­‰é‚è¼¯ã€‚

| å·¥å…· | åŠŸèƒ½ | é©ç”¨å ´æ™¯ |
|------|------|-----------|
| **spark-shell** | Scala / Python / R REPLï¼Œå¿«é€Ÿæ¸¬è©¦ | å°å‹æ¸¬è©¦ã€å­¸ç¿’ |
| **Notebook** | Jupyterã€Zeppelinã€Databricks Notebook | è³‡æ–™æ¢ç´¢ã€å¯è¦–åŒ–åˆ†æ |

> **ç‰¹é»**ï¼šå¿«é€Ÿé©—è­‰é‚è¼¯ï¼Œä½†**ä¸é©åˆé•·æ™‚é–“é‹è¡Œ**æˆ–å¤§è¦æ¨¡è¨ˆç®—ã€‚

---

### æäº¤æ‰¹æ¬¡ä»»å‹™ (Submit Job) ğŸš€
é©åˆæ­£å¼ç’°å¢ƒï¼Œå°‡ Spark Job æäº¤çµ¦å¢é›†é‹è¡Œã€‚

| å·¥å…· | åŠŸèƒ½ | é©ç”¨å ´æ™¯ |
|------|------|-----------|
| **spark-submit** | æœ€å¸¸ç”¨æ–¹å¼ï¼Œæäº¤ Application è‡³å¢é›† | Prod ETLã€æ‰¹æ¬¡è™•ç† |
| **Databricks** | é›²ç«¯ Notebook å¹³å°ï¼Œå…§å»º Spark é‹è¡Œç’°å¢ƒ | é›²ç«¯æ•¸æ“šè™•ç† |
| **REST API / Web UI** | æäº¤ã€ç›£æ§ã€ç®¡ç† Spark Job | è‡ªå‹•åŒ–èª¿åº¦ |

---

## Spark é‹ç®—æ¶æ§‹èˆ‡æäº¤æµç¨‹

Spark æ¡ç”¨ **Driver + Executor** æ¶æ§‹ï¼Œé€é **Cluster Manager** ç®¡ç†è³‡æºã€‚

### æ ¸å¿ƒå…ƒä»¶
| å…ƒä»¶ | é¡å‹ | åŠŸèƒ½ |
|------|------|------|
| **Client** | æäº¤ç«¯ | æäº¤ Jobï¼Œä¾‹å¦‚ `spark-submit` |
| **Driver** | JVM Process | ä»»å‹™èª¿åº¦ä¸­å¿ƒï¼Œè² è²¬ Stage åˆ†å‰²èˆ‡ Task åˆ†é… |
| **Executor** | JVM Process | åŸ·è¡Œ Tasksï¼Œè² è²¬è¨ˆç®—è³‡æ–™ |
| **Task** | Thread | Executor å…§åŸ·è¡Œçš„æœ€å°è¨ˆç®—å–®ä½ |
| **Cluster Manager** | è³‡æºç®¡ç†å™¨ | åˆ†é…å¢é›† CPU / Memory è³‡æºï¼Œå•Ÿå‹• Executors |

---

### Spark Job æäº¤æµç¨‹

![Job Submit Flow](chart1.png)

---

## Spark åŸ·è¡Œæ¨¡å¼èˆ‡ Cluster Manager

Spark æ”¯æ´å¤šç¨®åŸ·è¡Œæ¨¡å¼ï¼Œæ±ºå®š Driver èˆ‡ Executor çš„é‹è¡Œä½ç½®ã€‚

| æ¨¡å¼ | spark.master è¨­å®š | JVM Process æ•¸é‡ | Thread æ•¸é‡ | é©ç”¨å ´æ™¯ |
|------|--------------------|------------------|-------------|-----------|
| **Local[3]** | `local[3]` | 1 Driver + 1 Executor | 3 | æœ¬æ©Ÿæ¸¬è©¦ / æ¨¡æ“¬ä¸¦è¡Œ |
| **Local[*]** | `local[*]` | 1 Driver + 1 Executor | CPUæ ¸å¿ƒæ•¸ | å£“æ¸¬æˆ–å–®æ©Ÿæ¥µé™ |
| **Standalone** | `spark://host:7077` | å¤š Executors | å¤š Threads | Spark åŸç”Ÿå¢é›† |
| **YARN** | `yarn` | Container æ±ºå®š | å¤š Threads | Hadoop ç”Ÿæ…‹ |
| **Kubernetes** | `k8s://` | Pod æ±ºå®š | å¤š Threads | é›²ç«¯åŸç”Ÿ |
| **Mesos** | `mesos://` | å¤š Executors | å¤š Threads | å¤§å‹ä¼æ¥­å…±äº«å¢é›† |

---

## Local æ¨¡å¼ç¯„ä¾‹ï¼šspark.master=local[3]

### local[3] é‹è¡Œæ¶æ§‹åœ–

![local=3](chart2.png)

```text
- 1 Driver + 1 Executor JVM  
- Executor å…§ 3 Threads â†’ åŒæ™‚è™•ç† 3 Tasks  
- è‹¥ 12 Partitions â†’ Spark éœ€åˆ† 4 è¼ªåŸ·è¡Œ
```

---

## YARN æ¨¡å¼æ¶æ§‹åœ–

![YARN](chart3.png)

---

## Kubernetes æ¨¡å¼æ¶æ§‹åœ–

![Kubernetes](chart4.png)

---

## Spark å¯å˜—è©¦è³‡æºé…ç½®ç­–ç•¥

| æ¨¡å¼ | Driver ä½ç½® | Executor JVM æ•¸ | æ¯ Executor Threads | æœ€å¤§ä½µè¡Œåº¦ | é©ç”¨å ´æ™¯ |
|------|------------|-----------------|---------------------|-----------|-----------|
| **local[3]** | æœ¬æ©Ÿ | 1 | 3 | 3 | å°å‹æ¸¬è©¦ |
| **YARN** | ResourceManager | 4 | 4 | 16 | Hadoop ç”Ÿæ…‹ |
| **K8s** | Pod | 4 | 4 | 16 | é›²ç«¯åŸç”Ÿ |

**æœ€å¤§ä½µè¡Œåº¦å…¬å¼ï¼š**
```text
Max Concurrent Tasks = Executors Ã— Executor Cores
```

---

## Spark ä¸€äº›èª¿æ•´å»ºè­°

1. **Partition** å»ºè­°å¤§å° â‰ˆ 128MB  
2. **Executors Ã— Cores** â‰ˆ Partition æ•¸ / 2~3  
3. **Shuffle Partition** = Executors Ã— Cores Ã— 2  
4. é¿å…å–® Executor éå¤š Threads â†’ é™ä½ GC è² æ“”  
5. Production å»ºè­°é–‹å•Ÿå‹•æ…‹è³‡æºé…ç½®ï¼š  
```bash
--conf spark.dynamicAllocation.enabled=true
```

---

## ç¸½çµ

- **local[3]** â†’ 1 Executor JVM + 3 Threads â†’ é©åˆé–‹ç™¼èˆ‡æ¨¡æ“¬ä¸¦è¡Œ  
- Production â†’ å»ºè­°ä½¿ç”¨ **YARN / K8s / Standalone**  
- Spark æ•ˆèƒ½èª¿å„ªæ ¸å¿ƒä¸‰æ­¥ï¼š
    1. æ±ºå®š Partition æ•¸é‡
    2. è¨­å®š Executors Ã— Cores
    3. èª¿æ•´ Shuffle Partitions

---

## Reference
[PySpark - Apache Spark Programming in Python for beginners](https://www.udemy.com/course/apache-spark-programming-in-python-for-beginners/)

---

æœ€è¿‘æ‰“æ‹³è¢«æåˆ°è…¦è¢‹æœ‰é»ä¸éˆå…‰ ğŸ¤•  
å¯«æ–‡ç« éœ€è¦å’–å•¡ä¾†è£œè¡€ â˜•  
å¦‚æœä½ å–œæ­¡é€™ç¯‡å…§å®¹ï¼Œæ­¡è¿è«‹æˆ‘å–æ¯å’–å•¡ï¼  

Lately Iâ€™ve been punched a bit too much in boxing ğŸ¥Š  
My brain runs on coffee patches â˜•  
If you enjoyed this post, fuel me with a cup!  

ğŸ‘‰ [Buy Me a Coffee](https://buymeacoffee.com/james604s)

---